{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a24280e6-8607-487f-8617-aca2cc532af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/15 16:25:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "notebook_dir = os.getcwd()\n",
    "src_path = os.path.abspath(os.path.join(notebook_dir, '..', 'src'))\n",
    "sys.path.append(src_path)\n",
    "\n",
    "from utils.spark_utils import load_config, initialize_spark, insert_dataframe_to_sqlite, read_dataframe_from_sqlite\n",
    "from pyspark.sql.functions import col, count, avg, sum, format_number,current_date\n",
    "\n",
    "# Load configuration\n",
    "config = load_config(\"../configs/data_config.json\")\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = initialize_spark(config[\"spark\"][\"app_name\"], \"../jars/sqlite-jdbc-3.46.0.1.jar\")\n",
    "\n",
    "product_df = read_dataframe_from_sqlite(spark, \"product_sdf\", \"../data/raw/retail_data.db\",\"org.sqlite.JDBC\")\n",
    "store_df = read_dataframe_from_sqlite(spark, \"store_sdf\", \"../data/raw/retail_data.db\",\"org.sqlite.JDBC\")\n",
    "customer_df = read_dataframe_from_sqlite(spark, \"customer_sdf\", \"../data/raw/retail_data.db\",\"org.sqlite.JDBC\")\n",
    "time_df = read_dataframe_from_sqlite(spark, \"time_sdf\", \"../data/raw/retail_data.db\",\"org.sqlite.JDBC\")\n",
    "sales_df = read_dataframe_from_sqlite(spark, \"sales_sdf\", \"../data/raw/retail_data.db\",\"org.sqlite.JDBC\")\n",
    "supplier_df = read_dataframe_from_sqlite(spark, \"supplier_sdf\", \"../data/raw/retail_data.db\",\"org.sqlite.JDBC\")\n",
    "feedback_df = read_dataframe_from_sqlite(spark, \"feedback_sdf\", \"../data/raw/retail_data.db\",\"org.sqlite.JDBC\")\n",
    "loyalty_df = read_dataframe_from_sqlite(spark, \"loyalty_sdf\", \"../data/raw/retail_data.db\",\"org.sqlite.JDBC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35e52cd-e1bc-452f-b820-c4842f9658c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print schema for all DataFrames\n",
    "print(\"=== Product DataFrame Schema ===\")\n",
    "product_df.printSchema()\n",
    "\n",
    "print(\"=== Store DataFrame Schema ===\")\n",
    "store_df.printSchema()\n",
    "\n",
    "print(\"=== Customer DataFrame Schema ===\")\n",
    "customer_df.printSchema()\n",
    "\n",
    "print(\"=== Time DataFrame Schema ===\")\n",
    "time_df.printSchema()\n",
    "\n",
    "print(\"=== Sales DataFrame Schema ===\")\n",
    "sales_df.printSchema()\n",
    "\n",
    "print(\"=== Supplier DataFrame Schema ===\")\n",
    "supplier_df.printSchema()\n",
    "\n",
    "print(\"=== Feedback DataFrame Schema ===\")\n",
    "feedback_df.printSchema()\n",
    "\n",
    "print(\"=== Loyalty DataFrame Schema ===\")\n",
    "loyalty_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4f3d4b-773e-4563-ab41-1806bbf50d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import necessary functions\n",
    "from pyspark.sql.functions import count, avg, sum, col, countDistinct, format_number, year, datediff, current_date, max, min\n",
    "\n",
    "### 1. Product Analysis\n",
    "print(\"=== Product Analysis ===\")\n",
    "product_df.printSchema()\n",
    "\n",
    "# Basic stats\n",
    "total_products = product_df.select(\"Product_ID\").distinct().count()\n",
    "total_brands = product_df.select(\"Brand_Name\").distinct().count()\n",
    "total_categories = product_df.select(\"Category\").distinct().count()\n",
    "total_suppliers = product_df.select(\"Supplier_ID\").distinct().count()\n",
    "\n",
    "print(f\"Total Products: {total_products}\")\n",
    "print(f\"Total Brands: {total_brands}\")\n",
    "print(f\"Total Categories: {total_categories}\")\n",
    "print(f\"Total Suppliers: {total_suppliers}\")\n",
    "\n",
    "# Products and Avg. Price Across Categories\n",
    "print(\"Products and Average Price Across Categories\")\n",
    "category_statistics = product_df.groupBy(\"Category\").agg(\n",
    "    count(\"Product_ID\").alias(\"Product_Count\"),\n",
    "    format_number(avg(\"Price\"), 2).alias(\"Average_Price\")\n",
    ")\n",
    "category_statistics.show()\n",
    "\n",
    "# Brands by Product Count\n",
    "print(\"Brands by Product Count\")\n",
    "brand_statistics = product_df.groupBy(\"Brand_Name\").agg(\n",
    "    count(\"Product_ID\").alias(\"Product_Count\"),\n",
    "    format_number(avg(\"Price\"), 2).alias(\"Average_Price\")\n",
    ")\n",
    "brand_statistics.orderBy(col(\"Product_Count\").desc()).show(10)\n",
    "\n",
    "### 2. Store Analysis\n",
    "print(\"=== Store Analysis ===\")\n",
    "store_df.printSchema()\n",
    "\n",
    "# Basic stats\n",
    "total_stores = store_df.select(\"Store_ID\").distinct().count()\n",
    "total_store_locations = store_df.select(\"Store_Location\").distinct().count()\n",
    "\n",
    "print(f\"Total Stores: {total_stores}\")\n",
    "print(f\"Total Store Locations: {total_store_locations}\")\n",
    "\n",
    "# Stores and Avg. Size by Store Type\n",
    "print(\"Stores and Average Size by Store Type\")\n",
    "store_statistics = store_df.groupBy(\"Store_Type\").agg(\n",
    "    count(\"Store_ID\").alias(\"Store_Count\"),\n",
    "    format_number(avg(\"Store_Size\"), 2).alias(\"Average_Store_Size\")\n",
    ")\n",
    "store_statistics.show()\n",
    "\n",
    "# Store Size Distribution\n",
    "print(\"Store Size Distribution\")\n",
    "store_size_distribution = store_df.groupBy(\"Store_Size\").agg(\n",
    "    count(\"Store_ID\").alias(\"Store_Count\")\n",
    ")\n",
    "store_size_distribution.orderBy(col(\"Store_Size\").desc()).show()\n",
    "\n",
    "### 3. Customer Analysis\n",
    "print(\"=== Customer Analysis ===\")\n",
    "customer_df.printSchema()\n",
    "\n",
    "# Basic stats\n",
    "total_customers = customer_df.select(\"Customer_ID\").distinct().count()\n",
    "print(f\"Total Customers: {total_customers}\")\n",
    "\n",
    "# Customers by State\n",
    "print(\"Customer Distribution by State\")\n",
    "customer_state_stats = customer_df.groupBy(\"State\").agg(\n",
    "    count(\"Customer_ID\").alias(\"Customer_Count\")\n",
    ")\n",
    "customer_state_stats.show()\n",
    "\n",
    "# Gender Distribution\n",
    "print(\"Gender Distribution\")\n",
    "customer_df.groupBy(\"Gender\").count().show()\n",
    "\n",
    "# Age Analysis\n",
    "print(\"Customer Age Analysis\")\n",
    "current_year = year(current_date())\n",
    "customer_age_df = customer_df.withColumn(\"Age\", current_year - year(col(\"DOB\")))\n",
    "\n",
    "# Age Distribution\n",
    "print(\"Age Distribution\")\n",
    "age_distribution = customer_age_df.groupBy(\"Age\").agg(\n",
    "    count(\"Customer_ID\").alias(\"Customer_Count\")\n",
    ")\n",
    "age_distribution.orderBy(col(\"Age\").desc()).show()\n",
    "\n",
    "# Customer Join Date Distribution\n",
    "print(\"Customer Join Date Distribution\")\n",
    "customer_df.groupBy(year(\"Customer_Join_Date\").alias(\"Join_Year\")).count().orderBy(\"Join_Year\").show()\n",
    "\n",
    "### 4. Sales Analysis\n",
    "print(\"=== Sales Analysis ===\")\n",
    "sales_df.printSchema()\n",
    "\n",
    "# Basic stats\n",
    "total_transactions = sales_df.select(\"Transaction_ID\").distinct().count()\n",
    "print(f\"Total Transactions: {total_transactions}\")\n",
    "\n",
    "# Avg Transaction Value by Store\n",
    "print(\"Average Transaction Value by Store\")\n",
    "sales_statistics = sales_df.groupBy(\"Store_ID\").agg(\n",
    "    count(\"Transaction_ID\").alias(\"Transaction_Count\"),\n",
    "    format_number(avg(\"Sales_Amount\"), 2).alias(\"Average_Sales_Amount\"),\n",
    "    format_number(sum(\"Sales_Amount\"), 2).alias(\"Total_Sales\")\n",
    ")\n",
    "sales_statistics.show()\n",
    "\n",
    "# Top 10 Products Sold by Sales\n",
    "print(\"Top 10 Products Sold by Sales Amount\")\n",
    "product_sales = sales_df.groupBy(\"Product_ID\").agg(\n",
    "    format_number(sum(col(\"Sales_Amount\")), 2).alias(\"Total_Sales\")\n",
    ")\n",
    "top_selling_products = product_sales.orderBy(col(\"Total_Sales\").desc()).limit(10)\n",
    "top_selling_products.show()\n",
    "\n",
    "### 5. Feedback Analysis\n",
    "print(\"=== Feedback Analysis ===\")\n",
    "feedback_df.printSchema()\n",
    "\n",
    "# Basic stats\n",
    "total_feedbacks = feedback_df.select(\"Feedback_ID\").distinct().count()\n",
    "print(f\"Total Feedbacks: {total_feedbacks}\")\n",
    "\n",
    "# Top 10 Products with Highest Ratings\n",
    "print(\"Top 10 Products with Highest Ratings\")\n",
    "product_feedback_stats = feedback_df.groupBy(\"Product_ID\").agg(\n",
    "    count(\"Feedback_ID\").alias(\"Feedback_Count\"),\n",
    "    format_number(avg(col(\"Feedback_Rating\")), 2).alias(\"Average_Rating\")\n",
    ")\n",
    "top_rated_products = product_feedback_stats.orderBy(col(\"Average_Rating\").desc()).limit(10)\n",
    "top_rated_products.show()\n",
    "\n",
    "# Feedback Rating Distribution\n",
    "print(\"Feedback Rating Distribution\")\n",
    "feedback_rating_distribution = feedback_df.groupBy(\"Feedback_Rating\").agg(\n",
    "    count(\"Feedback_ID\").alias(\"Feedback_Count\")\n",
    ")\n",
    "feedback_rating_distribution.orderBy(col(\"Feedback_Rating\").desc()).show()\n",
    "\n",
    "### 6. Loyalty Analysis\n",
    "print(\"=== Loyalty Analysis ===\")\n",
    "loyalty_df.printSchema()\n",
    "\n",
    "# Membership Tier Distribution\n",
    "print(\"Membership Tier Distribution\")\n",
    "membership_tier_distribution = loyalty_df.groupBy(\"Membership_Tier\").agg(\n",
    "    count(\"Customer_ID\").alias(\"Customer_Count\"),\n",
    "    format_number(avg(\"Points_Earned\"), 2).alias(\"Average_Points_Earned\"),\n",
    "    format_number(avg(\"Points_Redeemed\"), 2).alias(\"Average_Points_Redeemed\")\n",
    ")\n",
    "membership_tier_distribution.show()\n",
    "\n",
    "### 7. Relationship Analysis\n",
    "\n",
    "# Relationship 1: Sales and Products\n",
    "print(\"=== Relationship Analysis: Sales and Products ===\")\n",
    "sales_product_df = sales_df.join(product_df, \"Product_ID\").dropDuplicates([\"Product_ID\", \"Store_ID\"])\n",
    "\n",
    "sales_product_analysis = sales_product_df.groupBy(\"Category\", \"Brand_Name\").agg(\n",
    "    format_number(sum(\"Sales_Amount\"), 2).alias(\"Total_Sales\"),\n",
    "    format_number(avg(\"Sales_Amount\"), 2).alias(\"Average_Sales_Amount\")\n",
    ")\n",
    "sales_product_analysis.show()\n",
    "\n",
    "# Relationship 2: Sales and Stores\n",
    "print(\"=== Relationship Analysis: Sales and Stores ===\")\n",
    "sales_store_analysis = sales_df.groupBy(\"Store_ID\").agg(\n",
    "    format_number(sum(\"Sales_Amount\"), 2).alias(\"Total_Sales\"),\n",
    "    format_number(avg(\"Sales_Amount\"), 2).alias(\"Average_Sales_Amount\")\n",
    ")\n",
    "sales_store_analysis.show()\n",
    "\n",
    "# Relationship 3: Feedback and Products with Sales\n",
    "print(\"=== Relationship Analysis: Feedback and Products with Sales ===\")\n",
    "feedback_sales_product_df = feedback_df.join(sales_df, \"Product_ID\").join(product_df, \"Product_ID\").dropDuplicates([\"Product_ID\"])\n",
    "\n",
    "feedback_product_analysis = feedback_sales_product_df.groupBy(\"Product_ID\", \"Product_Name\").agg(\n",
    "    format_number(avg(\"Feedback_Rating\"), 2).alias(\"Average_Rating\"),\n",
    "    format_number(sum(\"Sales_Amount\"), 2).alias(\"Total_Sales\")\n",
    ")\n",
    "feedback_product_analysis.orderBy(col(\"Average_Rating\").desc()).show(10)\n",
    "\n",
    "# Relationship 4: Loyalty and Sales\n",
    "print(\"=== Relationship Analysis: Loyalty and Sales ===\")\n",
    "loyalty_sales_df = loyalty_df.join(sales_df, \"Customer_ID\").dropDuplicates([\"Customer_ID\"])\n",
    "\n",
    "loyalty_sales_analysis = loyalty_sales_df.groupBy(\"Membership_Tier\").agg(\n",
    "    format_number(sum(\"Sales_Amount\"), 2).alias(\"Total_Sales\"),\n",
    "    format_number(avg(\"Sales_Amount\"), 2).alias(\"Average_Sales_Amount\")\n",
    ")\n",
    "loyalty_sales_analysis.show()\n",
    "\n",
    "# Relationship 5: Age and Sales\n",
    "print(\"=== Relationship Analysis: Age and Sales ===\")\n",
    "age_sales_df = customer_age_df.join(sales_df, \"Customer_ID\").dropDuplicates([\"Customer_ID\"])\n",
    "\n",
    "age_sales_analysis = age_sales_df.groupBy(\"Age\").agg(\n",
    "    format_number(sum(\"Sales_Amount\"), 2).alias(\"Total_Sales\"),\n",
    "    format_number(avg(\"Sales_Amount\"), 2).alias(\"Average_Sales_Amount\")\n",
    ")\n",
    "age_sales_analysis.orderBy(col(\"Age\").desc()).show()\n",
    "\n",
    "### 8. Summary Statistics for Customer Data\n",
    "print(\"=== Basic Statistics for Customer Data ===\")\n",
    "customer_df.describe().show()\n",
    "\n",
    "print(\"=== Top 10 Cities by Customer Count ===\")\n",
    "customer_df.groupBy(\"City\").count().orderBy(col(\"count\").desc()).show(10)\n",
    "\n",
    "print(\"=== Top 10 States by Customer Count ===\")\n",
    "customer_df.groupBy(\"State\").count().orderBy(col(\"count\").desc()).show(10)\n",
    "\n",
    "### 9. Aggregated Sales Data for Customers\n",
    "print(\"=== Aggregated Sales Data for Customers ===\")\n",
    "sales_agg_df = sales_df.groupBy(\"Customer_ID\").agg(\n",
    "    sum(\"Sales_Amount\").alias(\"Total_Spend\"),\n",
    "    countDistinct(\"Transaction_ID\").alias(\"Total_Transactions\"),\n",
    "    max(\"Date\").alias(\"Last_Purchase_Date\"),\n",
    "    min(\"Date\").alias(\"First_Purchase_Date\")\n",
    ")\n",
    "\n",
    "# Calculate Recency in days\n",
    "sales_agg_df = sales_agg_df.withColumn(\"Recency\", datediff(current_date(), col(\"Last_Purchase_Date\")))\n",
    "\n",
    "# Join with customer data\n",
    "customer_sales_df = customer_df.join(sales_agg_df, \"Customer_ID\", \"left\")\n",
    "\n",
    "# Show summary statistics of the new DataFrame\n",
    "print(\"=== Summary Statistics of Customer Spend, Transactions, and Recency ===\")\n",
    "customer_sales_df.describe([\"Total_Spend\", \"Total_Transactions\", \"Recency\"]).show()\n",
    "\n",
    "# Display the first few rows of the final DataFrame for review\n",
    "print(\"=== Sample Data from Customer Sales DataFrame ===\")\n",
    "customer_sales_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c28d42-2267-420f-a291-83ea76809b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import sum, countDistinct, max, datediff, current_date, mean\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.clustering import KMeans, BisectingKMeans, GaussianMixture\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"Customer Segmentation\").getOrCreate()\n",
    "\n",
    "# Load and preprocess data\n",
    "def load_data(spark):\n",
    "    customer_df = read_dataframe_from_sqlite(spark, \"customer_sdf\", \"../data/raw/retail_data.db\",\"org.sqlite.JDBC\")\n",
    "    sales_df = read_dataframe_from_sqlite(spark, \"sales_sdf\", \"../data/raw/retail_data.db\",\"org.sqlite.JDBC\")\n",
    "    return customer_df, sales_df\n",
    "\n",
    "def calculate_rfm(sales_df):\n",
    "    rfm_metrics = sales_df.groupBy(\"Customer_ID\").agg(\n",
    "        datediff(current_date(), max(\"Date\")).alias(\"Recency\"),\n",
    "        countDistinct(\"Transaction_ID\").alias(\"Frequency\"),\n",
    "        sum(\"Sales_Amount\").alias(\"Monetary\")\n",
    "    )\n",
    "    return rfm_metrics\n",
    "\n",
    "def preprocess_data(customer_rfm_df):\n",
    "    # Fill nulls with mean values for Recency, Frequency, and Monetary\n",
    "    mean_recency = customer_rfm_df.select(mean(\"Recency\")).first()[0]\n",
    "    mean_frequency = customer_rfm_df.select(mean(\"Frequency\")).first()[0]\n",
    "    mean_monetary = customer_rfm_df.select(mean(\"Monetary\")).first()[0]\n",
    "\n",
    "    customer_rfm_df = customer_rfm_df.fillna({\n",
    "        \"Recency\": mean_recency, \n",
    "        \"Frequency\": mean_frequency, \n",
    "        \"Monetary\": mean_monetary\n",
    "    })\n",
    "\n",
    "    # Assemble features into a vector\n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=[\"Recency\", \"Frequency\", \"Monetary\"], \n",
    "        outputCol=\"features\",\n",
    "        handleInvalid=\"skip\"  # Skip rows with null values\n",
    "    )\n",
    "    feature_vector = assembler.transform(customer_rfm_df)\n",
    "    \n",
    "    # Apply Standard Scaler\n",
    "    scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withMean=True, withStd=True)\n",
    "    scaled_data = scaler.fit(feature_vector).transform(feature_vector)\n",
    "    \n",
    "    return scaled_data\n",
    "\n",
    "def train_and_evaluate_model(model, scaled_data, model_name):\n",
    "    # Train the model\n",
    "    trained_model = model.fit(scaled_data)\n",
    "    \n",
    "    # Get cluster assignments\n",
    "    clusters = trained_model.transform(scaled_data)\n",
    "    \n",
    "    # Evaluate the model using Silhouette Score\n",
    "    evaluator = ClusteringEvaluator(featuresCol=\"scaled_features\", metricName=\"silhouette\", distanceMeasure=\"squaredEuclidean\")\n",
    "    silhouette = evaluator.evaluate(clusters)\n",
    "    \n",
    "    print(f\"{model_name} Silhouette Score: {silhouette}\")\n",
    "    \n",
    "    # Analyze each cluster's characteristics\n",
    "    cluster_summary = clusters.groupBy(\"prediction\").agg(\n",
    "        avg(\"Recency\").alias(\"Avg_Recency\"),\n",
    "        avg(\"Frequency\").alias(\"Avg_Frequency\"),\n",
    "        avg(\"Monetary\").alias(\"Avg_Monetary\")\n",
    "    )\n",
    "    cluster_summary.show()\n",
    "    \n",
    "    return silhouette, clusters\n",
    "\n",
    "# Load data\n",
    "customer_df, sales_df = load_data(spark)\n",
    "\n",
    "# Calculate RFM metrics\n",
    "rfm_df = calculate_rfm(sales_df)\n",
    "\n",
    "# Join RFM metrics with customer data\n",
    "customer_rfm_df = customer_df.join(rfm_df, \"Customer_ID\", \"left\")\n",
    "\n",
    "# Preprocess the data (assemble and scale features)\n",
    "scaled_data = preprocess_data(customer_rfm_df)\n",
    "\n",
    "# Dictionary to store models and their names\n",
    "models = {\n",
    "    \"KMeans\": KMeans(k=4, seed=1, featuresCol=\"scaled_features\"),\n",
    "    \"BisectingKMeans\": BisectingKMeans(k=4, seed=1, featuresCol=\"scaled_features\"),\n",
    "    \"GaussianMixture\": GaussianMixture(k=4, seed=1, featuresCol=\"scaled_features\")\n",
    "}\n",
    "\n",
    "# Dictionaries to store results\n",
    "silhouette_scores = {}\n",
    "cluster_results = {}\n",
    "\n",
    "# Train and evaluate each model\n",
    "for model_name, model in models.items():\n",
    "    silhouette_score, clusters = train_and_evaluate_model(model, scaled_data, model_name)\n",
    "    silhouette_scores[model_name] = silhouette_score\n",
    "    cluster_results[model_name] = clusters\n",
    "\n",
    "best_model_name = None\n",
    "best_score = float('-inf')\n",
    "\n",
    "for model_name, score in silhouette_scores.items():\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_model_name = model_name\n",
    "\n",
    "print(f\"Best Model: {best_model_name} with Silhouette Score = {best_score}\")\n",
    "\n",
    "# Visualize the clusters for the best-performing model\n",
    "pandas_df = cluster_results[best_model_name].select(\"Recency\", \"Frequency\", \"Monetary\", \"prediction\").toPandas()\n",
    "\n",
    "# Pairplot to visualize clusters\n",
    "sns.pairplot(pandas_df, hue=\"prediction\", palette=\"viridis\")\n",
    "plt.show()\n",
    "\n",
    "# Filter for customers in Cluster 1\n",
    "cluster_1_customers = cluster_results[\"BisectingKMeans\"].filter(col(\"prediction\") == 1).select(\"Customer_ID\")\n",
    "\n",
    "# Filter for customers in Cluster 0\n",
    "cluster_0_customers = cluster_results[\"BisectingKMeans\"].filter(col(\"prediction\") == 0).select(\"Customer_ID\")\n",
    "\n",
    "# Show the first few rows of each cluster\n",
    "print(\"Cluster 1 Customer IDs:\")\n",
    "cluster_1_customers.show()\n",
    "\n",
    "print(\"Cluster 0 Customer IDs:\")\n",
    "cluster_0_customers.show()\n",
    "\n",
    "# End Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414cdf91-680c-48a0-bcd3-fe8b7aae0eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import sum, countDistinct, max, datediff, current_date, mean, col, avg, year\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, StringIndexer\n",
    "from pyspark.ml.clustering import KMeans, BisectingKMeans, GaussianMixture\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"Customer Segmentation\").getOrCreate()\n",
    "\n",
    "# Function to load data\n",
    "def read_dataframe_from_sqlite(spark, table_name, db_path, driver):\n",
    "    return spark.read.format(\"jdbc\").option(\"url\", f\"jdbc:sqlite:{db_path}\")\\\n",
    "            .option(\"dbtable\", table_name).option(\"driver\", driver).load()\n",
    "\n",
    "# Load and preprocess data\n",
    "def load_data(spark):\n",
    "    customer_df = read_dataframe_from_sqlite(spark, \"customer_sdf\", \"../data/raw/retail_data.db\",\"org.sqlite.JDBC\")\n",
    "    sales_df = read_dataframe_from_sqlite(spark, \"sales_sdf\", \"../data/raw/retail_data.db\",\"org.sqlite.JDBC\")\n",
    "    product_df = read_dataframe_from_sqlite(spark, \"product_sdf\", \"../data/raw/retail_data.db\",\"org.sqlite.JDBC\")\n",
    "    store_df = read_dataframe_from_sqlite(spark, \"store_sdf\", \"../data/raw/retail_data.db\",\"org.sqlite.JDBC\")\n",
    "    loyalty_df = read_dataframe_from_sqlite(spark, \"loyalty_sdf\", \"../data/raw/retail_data.db\",\"org.sqlite.JDBC\")\n",
    "    return customer_df, sales_df, product_df, store_df, loyalty_df\n",
    "\n",
    "# Calculate RFM metrics\n",
    "def calculate_rfm(sales_df):\n",
    "    rfm_metrics = sales_df.groupBy(\"Customer_ID\").agg(\n",
    "        datediff(current_date(), max(\"Date\")).alias(\"Recency\"),\n",
    "        countDistinct(\"Transaction_ID\").alias(\"Frequency\"),\n",
    "        sum(\"Sales_Amount\").alias(\"Monetary\")\n",
    "    )\n",
    "    return rfm_metrics\n",
    "\n",
    "# Preprocess data (assemble and scale features)\n",
    "def preprocess_data(customer_rfm_df, feature_columns):\n",
    "    # Cast BigDecimal columns to DoubleType\n",
    "    for feature in feature_columns:\n",
    "        customer_rfm_df = customer_rfm_df.withColumn(feature, col(feature).cast(DoubleType()))\n",
    "    \n",
    "    # Check if DataFrame is empty\n",
    "    if customer_rfm_df.count() == 0:\n",
    "        raise ValueError(\"The DataFrame is empty. Cannot proceed with preprocessing.\")\n",
    "    \n",
    "    # Calculate mean values with error handling for None\n",
    "    mean_values = {}\n",
    "    for feature in feature_columns:\n",
    "        mean_value = customer_rfm_df.select(mean(feature)).first()[0]\n",
    "        if mean_value is None:\n",
    "            mean_value = 0.0  # Default to 0.0 if the mean is None\n",
    "        mean_values[feature] = mean_value\n",
    "\n",
    "    # Fill missing values with calculated mean values\n",
    "    customer_rfm_df = customer_rfm_df.fillna(mean_values)\n",
    "\n",
    "    # Assemble features into a vector\n",
    "    assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\", handleInvalid=\"skip\")\n",
    "    feature_vector = assembler.transform(customer_rfm_df)\n",
    "    \n",
    "    # Check if features contain any data\n",
    "    if feature_vector.select(\"features\").head()[0].size == 0:\n",
    "        raise ValueError(\"Feature vector is empty. Cannot proceed with scaling.\")\n",
    "    \n",
    "    # Apply Standard Scaler\n",
    "    scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withMean=True, withStd=True)\n",
    "    scaled_data = scaler.fit(feature_vector).transform(feature_vector)\n",
    "    \n",
    "    return scaled_data\n",
    "\n",
    "# Train and evaluate clustering model\n",
    "def train_and_evaluate_model(model, scaled_data, model_name, feature_columns):\n",
    "    # Check if DataFrame is empty\n",
    "    if scaled_data.count() == 0:\n",
    "        raise ValueError(f\"The DataFrame for {model_name} is empty. Cannot proceed with model training.\")\n",
    "    \n",
    "    trained_model = model.fit(scaled_data)\n",
    "    clusters = trained_model.transform(scaled_data)\n",
    "    \n",
    "    # Check if the number of clusters is greater than one\n",
    "    num_clusters = clusters.select(\"prediction\").distinct().count()\n",
    "    if num_clusters <= 1:\n",
    "        print(f\"{model_name}: Number of clusters is {num_clusters}. Cannot compute silhouette score.\")\n",
    "        return None, clusters\n",
    "    \n",
    "    evaluator = ClusteringEvaluator(featuresCol=\"scaled_features\", metricName=\"silhouette\", distanceMeasure=\"squaredEuclidean\")\n",
    "    silhouette = evaluator.evaluate(clusters)\n",
    "    \n",
    "    print(f\"{model_name} Silhouette Score: {silhouette}\")\n",
    "    \n",
    "    # Dynamically aggregate based on available features\n",
    "    agg_exprs = [avg(col(feature)).alias(f\"Avg_{feature}\") for feature in feature_columns]\n",
    "    \n",
    "    cluster_summary = clusters.groupBy(\"prediction\").agg(*agg_exprs)\n",
    "    cluster_summary.show()\n",
    "    \n",
    "    return silhouette, clusters\n",
    "\n",
    "# Product Purchase Segmentation\n",
    "def product_purchase_segmentation(sales_df, product_df):\n",
    "    sales_product_df = sales_df.join(product_df, \"Product_ID\")\n",
    "    customer_product_metrics = sales_product_df.groupBy(\"Customer_ID\", \"Category\").agg(\n",
    "        countDistinct(\"Transaction_ID\").alias(\"Purchase_Count\"),\n",
    "        sum(\"Sales_Amount\").alias(\"Total_Spending\")\n",
    "    )\n",
    "    return customer_product_metrics\n",
    "\n",
    "# Store Visit Segmentation\n",
    "def store_visit_segmentation(sales_df, store_df):\n",
    "    # Perform the join with the condition specified\n",
    "    sales_store_df = sales_df.join(store_df, sales_df[\"Store_ID\"] == store_df[\"Store_Location\"], \"left\")\n",
    "    \n",
    "    # Group by Customer_ID and Store_Type to calculate visit frequency and total spending\n",
    "    customer_store_metrics = sales_store_df.groupBy(\"Customer_ID\", \"Store_Type\").agg(\n",
    "        countDistinct(\"Transaction_ID\").alias(\"Visit_Frequency\"),\n",
    "        sum(\"Sales_Amount\").alias(\"Total_Spending\")\n",
    "    )\n",
    "    return customer_store_metrics\n",
    "\n",
    "# Demographic Segmentation with String Indexing\n",
    "def demographic_segmentation(customer_df):\n",
    "    customer_df = customer_df.withColumn(\"Age\", year(current_date()) - year(customer_df[\"DOB\"]))\n",
    "\n",
    "    # Convert categorical string columns to numerical indices\n",
    "    indexers = [StringIndexer(inputCol=col, outputCol=col+\"_index\").fit(customer_df) for col in [\"Gender\", \"City\", \"State\"]]\n",
    "    for indexer in indexers:\n",
    "        customer_df = indexer.transform(customer_df)\n",
    "    \n",
    "    return customer_df\n",
    "\n",
    "# Loyalty Program Segmentation\n",
    "def loyalty_program_segmentation(loyalty_df):\n",
    "    return loyalty_df\n",
    "\n",
    "# Load data\n",
    "customer_df, sales_df, product_df, store_df, loyalty_df = load_data(spark)\n",
    "\n",
    "# RFM Segmentation\n",
    "rfm_df = calculate_rfm(sales_df)\n",
    "customer_rfm_df = customer_df.join(rfm_df, \"Customer_ID\", \"left\")\n",
    "scaled_data_rfm = preprocess_data(customer_rfm_df, [\"Recency\", \"Frequency\", \"Monetary\"])\n",
    "\n",
    "# Product Purchase Segmentation\n",
    "product_metrics_df = product_purchase_segmentation(sales_df, product_df)\n",
    "scaled_data_product = preprocess_data(product_metrics_df, [\"Purchase_Count\", \"Total_Spending\"])\n",
    "\n",
    "# Store Visit Segmentation\n",
    "store_metrics_df = store_visit_segmentation(sales_df, store_df)\n",
    "print(\"Store Metrics DataFrame:\")\n",
    "store_metrics_df.show()  # Inspect the DataFrame\n",
    "\n",
    "if store_metrics_df.count() == 0:\n",
    "    print(\"No data available for store visit segmentation.\")\n",
    "else:\n",
    "    scaled_data_store = preprocess_data(store_metrics_df, [\"Visit_Frequency\", \"Total_Spending\"])\n",
    "\n",
    "# Demographic Segmentation\n",
    "demographic_df = demographic_segmentation(customer_df)\n",
    "scaled_data_demographic = preprocess_data(demographic_df, [\"Age\", \"Gender_index\", \"City_index\", \"State_index\"])\n",
    "\n",
    "# Loyalty Program Segmentation\n",
    "loyalty_df = loyalty_program_segmentation(loyalty_df)\n",
    "scaled_data_loyalty = preprocess_data(loyalty_df, [\"Points_Earned\", \"Points_Redeemed\", \"Membership_Tier\"])\n",
    "\n",
    "# Dictionary to store models and their names\n",
    "models = {\n",
    "    \"KMeans\": KMeans(k=4, seed=1, featuresCol=\"scaled_features\"),\n",
    "    \"BisectingKMeans\": BisectingKMeans(k=4, seed=1, featuresCol=\"scaled_features\"),\n",
    "    \"GaussianMixture\": GaussianMixture(k=4, seed=1, featuresCol=\"scaled_features\")\n",
    "}\n",
    "\n",
    "# Train and evaluate each model for each segmentation\n",
    "segmentation_data = {\n",
    "    \"RFM\": (scaled_data_rfm, [\"Recency\", \"Frequency\", \"Monetary\"]),\n",
    "    \"Product Purchase\": (scaled_data_product, [\"Purchase_Count\", \"Total_Spending\"]),\n",
    "    \"Store Visit\": (scaled_data_store if 'scaled_data_store' in locals() else None, [\"Visit_Frequency\", \"Total_Spending\"]),\n",
    "    \"Demographics\": (scaled_data_demographic, [\"Age\", \"Gender_index\", \"City_index\", \"State_index\"]),\n",
    "    \"Loyalty Program\": (scaled_data_loyalty, [\"Points_Earned\", \"Points_Redeemed\", \"Membership_Tier\"])\n",
    "}\n",
    "\n",
    "best_models = {}\n",
    "\n",
    "for seg_name, (data, feature_columns) in segmentation_data.items():\n",
    "    if data is not None and data.count() > 0:\n",
    "        silhouette_scores = {}\n",
    "        cluster_results = {}\n",
    "        best_model_name = None\n",
    "        best_score = float('-inf')\n",
    "        \n",
    "        for model_name, model in models.items():\n",
    "            silhouette_score, clusters = train_and_evaluate_model(model, data, f\"{seg_name} - {model_name}\", feature_columns)\n",
    "            if silhouette_score is not None:  # Only consider valid silhouette scores\n",
    "                silhouette_scores[model_name] = silhouette_score\n",
    "                cluster_results[model_name] = clusters\n",
    "                \n",
    "                if silhouette_score > best_score:\n",
    "                    best_score = silhouette_score\n",
    "                    best_model_name = model_name\n",
    "        \n",
    "        if best_model_name is not None:\n",
    "            best_models[seg_name] = {\n",
    "                \"model_name\": best_model_name,\n",
    "                \"silhouette_score\": best_score,\n",
    "                \"clusters\": cluster_results[best_model_name]\n",
    "            }\n",
    "            print(f\"Best Model for {seg_name}: {best_model_name} with Silhouette Score = {best_score}\")\n",
    "        else:\n",
    "            print(f\"No valid clustering model found for {seg_name}.\")\n",
    "    else:\n",
    "        print(f\"No data available for segmentation: {seg_name}\")\n",
    "\n",
    "# Visualize clusters for the best RFM model\n",
    "if \"RFM\" in best_models:\n",
    "    rfm_clusters = best_models[\"RFM\"][\"clusters\"]\n",
    "    pandas_df = rfm_clusters.select(\"Recency\", \"Frequency\", \"Monetary\", \"prediction\").toPandas()\n",
    "    \n",
    "    sns.pairplot(pandas_df, hue=\"prediction\", palette=\"viridis\")\n",
    "    plt.show()\n",
    "\n",
    "# End Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9856762-2a77-405f-aeba-04c7fe95dc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import countDistinct, sum, col\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"Behavioral Segmentation\").getOrCreate()\n",
    "\n",
    "# Load and preprocess data\n",
    "def load_data(spark):\n",
    "    customer_df = read_dataframe_from_sqlite(spark, \"customer_sdf\", \"../data/raw/retail_data.db\",\"org.sqlite.JDBC\")\n",
    "    sales_df = read_dataframe_from_sqlite(spark, \"sales_sdf\", \"../data/raw/retail_data.db\",\"org.sqlite.JDBC\")\n",
    "    product_df = read_dataframe_from_sqlite(spark, \"product_sdf\", \"../data/raw/retail_data.db\",\"org.sqlite.JDBC\")\n",
    "    return customer_df, sales_df, product_df\n",
    "\n",
    "# Product Variety and Spending Segmentation\n",
    "def product_variety_segmentation(sales_df, product_df):\n",
    "    # Join Sales and Product DataFrames\n",
    "    sales_product_df = sales_df.join(product_df, \"Product_ID\")\n",
    "    \n",
    "    # Aggregate data: calculate the variety of products purchased and the total spending\n",
    "    customer_behavioral_metrics = sales_product_df.groupBy(\"Customer_ID\").agg(\n",
    "        countDistinct(\"Category\").alias(\"Product_Variety\"),\n",
    "        sum(\"Sales_Amount\").alias(\"Total_Spending\")\n",
    "    )\n",
    "    \n",
    "    return customer_behavioral_metrics\n",
    "\n",
    "# Preprocess data (assemble and scale features)\n",
    "def preprocess_data(customer_behavioral_df, feature_columns):\n",
    "    # Check if DataFrame is empty\n",
    "    if customer_behavioral_df.count() == 0:\n",
    "        raise ValueError(\"The DataFrame is empty. Cannot proceed with preprocessing.\")\n",
    "    \n",
    "    # Fill missing values with zeros (if any)\n",
    "    customer_behavioral_df = customer_behavioral_df.fillna(0)\n",
    "\n",
    "    # Assemble features into a vector\n",
    "    assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\", handleInvalid=\"skip\")\n",
    "    feature_vector = assembler.transform(customer_behavioral_df)\n",
    "    \n",
    "    # Apply Standard Scaler\n",
    "    scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withMean=True, withStd=True)\n",
    "    scaled_data = scaler.fit(feature_vector).transform(feature_vector)\n",
    "    \n",
    "    return scaled_data\n",
    "\n",
    "# Train and evaluate clustering model\n",
    "def train_and_evaluate_model(model, scaled_data, model_name):\n",
    "    # Train the model\n",
    "    trained_model = model.fit(scaled_data)\n",
    "    clusters = trained_model.transform(scaled_data)\n",
    "    \n",
    "    # Evaluate the model using Silhouette Score\n",
    "    evaluator = ClusteringEvaluator(featuresCol=\"scaled_features\", metricName=\"silhouette\", distanceMeasure=\"squaredEuclidean\")\n",
    "    silhouette = evaluator.evaluate(clusters)\n",
    "    \n",
    "    print(f\"{model_name} Silhouette Score: {silhouette}\")\n",
    "    \n",
    "    # Analyze each cluster's characteristics\n",
    "    cluster_summary = clusters.groupBy(\"prediction\").agg(\n",
    "        avg(\"Product_Variety\").alias(\"Avg_Product_Variety\"),\n",
    "        avg(\"Total_Spending\").alias(\"Avg_Total_Spending\")\n",
    "    )\n",
    "    cluster_summary.show()\n",
    "    \n",
    "    return silhouette, clusters\n",
    "\n",
    "# Load data\n",
    "customer_df, sales_df, product_df = load_data(spark)\n",
    "\n",
    "# Perform Behavioral Segmentation\n",
    "behavioral_df = product_variety_segmentation(sales_df, product_df)\n",
    "scaled_data_behavioral = preprocess_data(behavioral_df, [\"Product_Variety\", \"Total_Spending\"])\n",
    "\n",
    "# Apply KMeans Clustering\n",
    "kmeans = KMeans(k=4, seed=1, featuresCol=\"scaled_features\")\n",
    "silhouette_score, clusters = train_and_evaluate_model(kmeans, scaled_data_behavioral, \"Behavioral Segmentation\")\n",
    "\n",
    "# Visualize the clusters\n",
    "pandas_df = clusters.select(\"Product_Variety\", \"Total_Spending\", \"prediction\").toPandas()\n",
    "sns.pairplot(pandas_df, hue=\"prediction\", palette=\"viridis\")\n",
    "plt.show()\n",
    "\n",
    "# End Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3f3843-3f8c-4d84-8960-bd41431779ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import sum, countDistinct, max, datediff, current_date, mean, col, avg, year, when, count\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, StringIndexer\n",
    "from pyspark.ml.clustering import KMeans, BisectingKMeans, GaussianMixture\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"Comprehensive Customer Segmentation\").getOrCreate()\n",
    "\n",
    "# Function to load data\n",
    "def read_dataframe_from_sqlite(spark, table_name, db_path, driver):\n",
    "    return spark.read.format(\"jdbc\").option(\"url\", f\"jdbc:sqlite:{db_path}\")\\\n",
    "            .option(\"dbtable\", table_name).option(\"driver\", driver).load()\n",
    "\n",
    "# Load and preprocess data\n",
    "def load_data(spark):\n",
    "    customer_df = read_dataframe_from_sqlite(spark, \"customer_sdf\", \"../data/raw/retail_data.db\",\"org.sqlite.JDBC\")\n",
    "    sales_df = read_dataframe_from_sqlite(spark, \"sales_sdf\", \"../data/raw/retail_data.db\",\"org.sqlite.JDBC\")\n",
    "    product_df = read_dataframe_from_sqlite(spark, \"product_sdf\", \"../data/raw/retail_data.db\",\"org.sqlite.JDBC\")\n",
    "    store_df = read_dataframe_from_sqlite(spark, \"store_sdf\", \"../data/raw/retail_data.db\",\"org.sqlite.JDBC\")\n",
    "    loyalty_df = read_dataframe_from_sqlite(spark, \"loyalty_sdf\", \"../data/raw/retail_data.db\",\"org.sqlite.JDBC\")\n",
    "    feedback_df = read_dataframe_from_sqlite(spark, \"feedback_sdf\", \"../data/raw/retail_data.db\",\"org.sqlite.JDBC\")\n",
    "    return customer_df, sales_df, product_df, store_df, loyalty_df, feedback_df\n",
    "\n",
    "# Calculate RFM metrics with Time\n",
    "def calculate_rfm_with_time(sales_df):\n",
    "    rfm_metrics = sales_df.groupBy(\"Customer_ID\").agg(\n",
    "        datediff(current_date(), max(\"Date\")).alias(\"Recency\"),\n",
    "        countDistinct(\"Transaction_ID\").alias(\"Frequency\"),\n",
    "        sum(\"Sales_Amount\").alias(\"Monetary\"),\n",
    "        datediff(current_date(), min(\"Date\")).alias(\"Customer_Age\")  # Add customer tenure/age\n",
    "    )\n",
    "    return rfm_metrics\n",
    "\n",
    "# Loyalty Program Engagement Segmentation\n",
    "def loyalty_program_engagement_segmentation(loyalty_df):\n",
    "    loyalty_metrics = loyalty_df.groupBy(\"Customer_ID\", \"Membership_Tier\").agg(\n",
    "        sum(\"Points_Earned\").alias(\"Total_Points_Earned\"),\n",
    "        sum(col(\"Points_Redeemed\").cast(DoubleType())).alias(\"Total_Points_Redeemed\")\n",
    "    )\n",
    "\n",
    "    # Verify that the Membership_Tier column exists\n",
    "    if \"Membership_Tier\" not in loyalty_metrics.columns:\n",
    "        raise ValueError(\"The column 'Membership_Tier' does not exist in the DataFrame.\")\n",
    "    \n",
    "    # Include Membership_Tier as a feature\n",
    "    indexer = StringIndexer(inputCol=\"Membership_Tier\", outputCol=\"Membership_Tier_Index\")\n",
    "    loyalty_metrics = indexer.fit(loyalty_metrics).transform(loyalty_metrics)\n",
    "    return loyalty_metrics\n",
    "\n",
    "# Customer Lifecycle Segmentation\n",
    "def customer_lifecycle_segmentation(rfm_df):\n",
    "    rfm_df = rfm_df.withColumn(\n",
    "        \"Lifecycle_Segment\",\n",
    "        when(col(\"Recency\") <= 30, \"Active\").when(col(\"Recency\") <= 90, \"Warm\").otherwise(\"Inactive\")\n",
    "    )\n",
    "    indexer = StringIndexer(inputCol=\"Lifecycle_Segment\", outputCol=\"Lifecycle_Segment_Index\")\n",
    "    rfm_df = indexer.fit(rfm_df).transform(rfm_df)\n",
    "    return rfm_df\n",
    "\n",
    "# Product Affinity Segmentation\n",
    "def product_affinity_segmentation(sales_df, product_df):\n",
    "    product_affinity_df = sales_df.join(product_df, \"Product_ID\").groupBy(\"Customer_ID\", \"Category\").agg(\n",
    "        countDistinct(\"Transaction_ID\").alias(\"Category_Purchase_Count\"),\n",
    "        sum(\"Sales_Amount\").alias(\"Category_Spending\")\n",
    "    )\n",
    "    return product_affinity_df\n",
    "\n",
    "# Store Loyalty Segmentation\n",
    "def store_loyalty_segmentation(sales_df, store_df):\n",
    "    store_loyalty_df = sales_df.join(store_df, sales_df[\"Store_ID\"] == store_df[\"Store_Location\"], \"left\").groupBy(\n",
    "        \"Customer_ID\", \"Store_Type\").agg(\n",
    "        countDistinct(\"Transaction_ID\").alias(\"Store_Visit_Frequency\"),\n",
    "        sum(\"Sales_Amount\").alias(\"Store_Total_Spending\")\n",
    "    )\n",
    "    return store_loyalty_df\n",
    "\n",
    "# Feedback Sentiment Segmentation\n",
    "def feedback_sentiment_segmentation(feedback_df):\n",
    "    feedback_sentiment_df = feedback_df.groupBy(\"Customer_ID\").agg(\n",
    "        avg(\"Feedback_Rating\").alias(\"Avg_Sentiment_Score\"),  # Use Feedback_Rating as the sentiment score\n",
    "        count(\"Feedback_ID\").alias(\"Total_Feedbacks\")\n",
    "    )\n",
    "    return feedback_sentiment_df\n",
    "\n",
    "# Preprocess data (assemble and scale features)\n",
    "def preprocess_data(customer_df, feature_columns):\n",
    "    # Cast BigDecimal columns to DoubleType\n",
    "    for feature in feature_columns:\n",
    "        customer_df = customer_df.withColumn(feature, col(feature).cast(DoubleType()))\n",
    "    \n",
    "    # Check if DataFrame is empty\n",
    "    if customer_df.count() == 0:\n",
    "        raise ValueError(\"The DataFrame is empty. Cannot proceed with preprocessing.\")\n",
    "    \n",
    "    # Calculate mean values with error handling for None\n",
    "    mean_values = {}\n",
    "    for feature in feature_columns:\n",
    "        mean_value = customer_df.select(mean(feature)).first()[0]\n",
    "        if mean_value is None:\n",
    "            mean_value = 0.0  # Default to 0.0 if the mean is None\n",
    "        mean_values[feature] = mean_value\n",
    "\n",
    "    # Fill missing values with calculated mean values\n",
    "    customer_df = customer_df.fillna(mean_values)\n",
    "\n",
    "    # Assemble features into a vector\n",
    "    assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\", handleInvalid=\"skip\")\n",
    "    feature_vector = assembler.transform(customer_df)\n",
    "    \n",
    "    # Check if features contain any data\n",
    "    if feature_vector.select(\"features\").head()[0].size == 0:\n",
    "        raise ValueError(\"Feature vector is empty. Cannot proceed with scaling.\")\n",
    "    \n",
    "    # Apply Standard Scaler\n",
    "    scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withMean=True, withStd=True)\n",
    "    scaled_data = scaler.fit(feature_vector).transform(feature_vector)\n",
    "    \n",
    "    return scaled_data\n",
    "\n",
    "# Train and evaluate clustering model\n",
    "def train_and_evaluate_model(model, scaled_data, model_name, feature_columns):\n",
    "    # Check if DataFrame is empty\n",
    "    if scaled_data.count() == 0:\n",
    "        raise ValueError(f\"The DataFrame for {model_name} is empty. Cannot proceed with model training.\")\n",
    "    \n",
    "    trained_model = model.fit(scaled_data)\n",
    "    clusters = trained_model.transform(scaled_data)\n",
    "    \n",
    "    # Check if the number of clusters is greater than one\n",
    "    num_clusters = clusters.select(\"prediction\").distinct().count()\n",
    "    if num_clusters <= 1:\n",
    "        print(f\"{model_name}: Number of clusters is {num_clusters}. Cannot compute silhouette score.\")\n",
    "        return None, clusters\n",
    "    \n",
    "    evaluator = ClusteringEvaluator(featuresCol=\"scaled_features\", metricName=\"silhouette\", distanceMeasure=\"squaredEuclidean\")\n",
    "    silhouette = evaluator.evaluate(clusters)\n",
    "    \n",
    "    print(f\"{model_name} Silhouette Score: {silhouette}\")\n",
    "    \n",
    "    # Dynamically aggregate based on available features\n",
    "    agg_exprs = [avg(col(feature)).alias(f\"Avg_{feature}\") for feature in feature_columns]\n",
    "    \n",
    "    cluster_summary = clusters.groupBy(\"prediction\").agg(*agg_exprs)\n",
    "    cluster_summary.show()\n",
    "    \n",
    "    return silhouette, clusters\n",
    "\n",
    "# Function to plot clusters\n",
    "def plot_clusters(clusters, feature_columns, seg_name):\n",
    "    # Convert to Pandas DataFrame for plotting\n",
    "    pandas_df = clusters.select(*feature_columns, \"prediction\", \"Customer_ID\").toPandas()\n",
    "    \n",
    "    # Plot pairplot if there are more than one feature\n",
    "    if len(feature_columns) > 1:\n",
    "        sns.pairplot(pandas_df, hue=\"prediction\", palette=\"viridis\")\n",
    "        plt.suptitle(f\"Cluster Plot for {seg_name}\", y=1.02)\n",
    "        plt.show()\n",
    "    else:\n",
    "        # Simple scatter plot if there's only one feature\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.scatterplot(x=feature_columns[0], y=\"prediction\", data=pandas_df, hue=\"prediction\", palette=\"viridis\")\n",
    "        plt.title(f\"Cluster Plot for {seg_name}\")\n",
    "        plt.show()\n",
    "\n",
    "# Load data\n",
    "customer_df, sales_df, product_df, store_df, loyalty_df, feedback_df = load_data(spark)\n",
    "\n",
    "# RFM Segmentation Enhanced with Time\n",
    "rfm_df = calculate_rfm_with_time(sales_df)\n",
    "customer_rfm_df = customer_df.join(rfm_df, \"Customer_ID\", \"left\")\n",
    "scaled_data_rfm = preprocess_data(customer_rfm_df, [\"Recency\", \"Frequency\", \"Monetary\", \"Customer_Age\"])\n",
    "\n",
    "# Loyalty Program Engagement Segmentation\n",
    "loyalty_metrics_df = loyalty_program_engagement_segmentation(loyalty_df)\n",
    "scaled_data_loyalty = preprocess_data(loyalty_metrics_df, [\"Total_Points_Earned\", \"Total_Points_Redeemed\", \"Membership_Tier_Index\"])\n",
    "\n",
    "# Customer Lifecycle Segmentation\n",
    "lifecycle_df = customer_lifecycle_segmentation(rfm_df)\n",
    "scaled_data_lifecycle = preprocess_data(lifecycle_df, [\"Recency\", \"Frequency\", \"Monetary\", \"Lifecycle_Segment_Index\"])\n",
    "\n",
    "# Product Affinity Segmentation\n",
    "product_affinity_df = product_affinity_segmentation(sales_df, product_df)\n",
    "scaled_data_product_affinity = preprocess_data(product_affinity_df, [\"Category_Purchase_Count\", \"Category_Spending\"])\n",
    "\n",
    "# Store Loyalty Segmentation\n",
    "store_loyalty_df = store_loyalty_segmentation(sales_df, store_df)\n",
    "scaled_data_store_loyalty = preprocess_data(store_loyalty_df, [\"Store_Visit_Frequency\", \"Store_Total_Spending\"])\n",
    "\n",
    "# Feedback Sentiment Segmentation\n",
    "feedback_sentiment_df = feedback_sentiment_segmentation(feedback_df)\n",
    "scaled_data_feedback_sentiment = preprocess_data(feedback_sentiment_df, [\"Avg_Sentiment_Score\", \"Total_Feedbacks\"])\n",
    "\n",
    "# Dictionary to store models and their names\n",
    "models = {\n",
    "    \"KMeans\": KMeans(k=4, seed=1, featuresCol=\"scaled_features\"),\n",
    "    \"BisectingKMeans\": BisectingKMeans(k=4, seed=1, featuresCol=\"scaled_features\"),\n",
    "    \"GaussianMixture\": GaussianMixture(k=4, seed=1, featuresCol=\"scaled_features\")\n",
    "}\n",
    "\n",
    "# Train and evaluate each model for each segmentation\n",
    "segmentation_data = {\n",
    "    \"RFM with Time\": (scaled_data_rfm, [\"Recency\", \"Frequency\", \"Monetary\", \"Customer_Age\"]),\n",
    "    \"Loyalty Program Engagement\": (scaled_data_loyalty, [\"Total_Points_Earned\", \"Total_Points_Redeemed\", \"Membership_Tier_Index\"]),\n",
    "    \"Customer Lifecycle\": (scaled_data_lifecycle, [\"Recency\", \"Frequency\", \"Monetary\", \"Lifecycle_Segment_Index\"]),\n",
    "    \"Product Affinity\": (scaled_data_product_affinity, [\"Category_Purchase_Count\", \"Category_Spending\"]),\n",
    "    \"Store Loyalty\": (scaled_data_store_loyalty, [\"Store_Visit_Frequency\", \"Store_Total_Spending\"]),\n",
    "    \"Feedback Sentiment\": (scaled_data_feedback_sentiment, [\"Avg_Sentiment_Score\", \"Total_Feedbacks\"])\n",
    "}\n",
    "\n",
    "best_models = {}\n",
    "segmented_customers = {}\n",
    "\n",
    "for seg_name, (data, feature_columns) in segmentation_data.items():\n",
    "    if data is not None and data.count() > 0:\n",
    "        silhouette_scores = {}\n",
    "        cluster_results = {}\n",
    "        best_model_name = None\n",
    "        best_score = float('-inf')\n",
    "        \n",
    "        for model_name, model in models.items():\n",
    "            silhouette_score, clusters = train_and_evaluate_model(model, data, f\"{seg_name} - {model_name}\", feature_columns)\n",
    "            if silhouette_score is not None:  # Only consider valid silhouette scores\n",
    "                silhouette_scores[model_name] = silhouette_score\n",
    "                cluster_results[model_name] = clusters\n",
    "                \n",
    "                if silhouette_score > best_score:\n",
    "                    best_score = silhouette_score\n",
    "                    best_model_name = model_name\n",
    "        \n",
    "        if best_model_name is not None:\n",
    "            best_models[seg_name] = {\n",
    "                \"model_name\": best_model_name,\n",
    "                \"silhouette_score\": best_score,\n",
    "                \"clusters\": cluster_results[best_model_name]\n",
    "            }\n",
    "            print(f\"Best Model for {seg_name}: {best_model_name} with Silhouette Score = {best_score}\")\n",
    "            \n",
    "            # Plot clusters\n",
    "            plot_clusters(best_models[seg_name][\"clusters\"], feature_columns, seg_name)\n",
    "            \n",
    "            # Retrieve and store customer IDs and their clusters\n",
    "            customer_clusters = best_models[seg_name][\"clusters\"].select(\"Customer_ID\", \"prediction\").toPandas()\n",
    "            segmented_customers[seg_name] = customer_clusters\n",
    "            print(customer_clusters.head())  # Display the first few rows\n",
    "        else:\n",
    "            print(f\"No valid clustering model found for {seg_name}.\")\n",
    "    else:\n",
    "        print(f\"No data available for segmentation: {seg_name}\")\n",
    "\n",
    "# End Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e116d75-dbef-493c-89a2-5f2f923e188c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting segmentation: RFM_with_Time\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMeans (k=2) Silhouette Score: 0.5055794891292529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMeans (k=3) Silhouette Score: 0.5086703340672766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMeans (k=4) Silhouette Score: 0.5526579558454616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.                                     \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: reentrant call inside <_io.BufferedReader name=78>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/socket.py\", line 708, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/pyspark/context.py\", line 381, in signal_handler\n",
      "    self.cancelAllJobs()\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/pyspark/context.py\", line 2446, in cancelAllJobs\n",
      "    self._jsc.sc().cancelAllJobs()\n",
      "    ^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "                   ^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/py4j/protocol.py\", line 334, in get_return_value\n",
      "    raise Py4JError(\n",
      "py4j.protocol.Py4JError: An error occurred while calling o11059.sc\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "[Stage 778:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling o12412.fit",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 193\u001b[0m\n\u001b[1;32m    190\u001b[0m best_summary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;66;03m# Grid Search for KMeans\u001b[39;00m\n\u001b[0;32m--> 193\u001b[0m kmeans_results \u001b[38;5;241m=\u001b[39m grid_search_kmeans(data, feature_columns, k_values)\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# Save KMeans results\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, silhouette, clusters, summary \u001b[38;5;129;01min\u001b[39;00m kmeans_results:\n",
      "Cell \u001b[0;32mIn[7], line 155\u001b[0m, in \u001b[0;36mgrid_search_kmeans\u001b[0;34m(data, feature_columns, k_values)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m k_values:\n\u001b[1;32m    154\u001b[0m     kmeans \u001b[38;5;241m=\u001b[39m KMeans(k\u001b[38;5;241m=\u001b[39mk, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, featuresCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscaled_features\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 155\u001b[0m     silhouette, clusters, cluster_summary \u001b[38;5;241m=\u001b[39m train_and_evaluate_model(kmeans, data, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKMeans (k=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m, feature_columns)\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m silhouette \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    158\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend((k, silhouette, clusters, cluster_summary))\n",
      "Cell \u001b[0;32mIn[13], line 118\u001b[0m, in \u001b[0;36mtrain_and_evaluate_model\u001b[0;34m(model, scaled_data, model_name, feature_columns)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m scaled_data\u001b[38;5;241m.\u001b[39mcount() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe DataFrame for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is empty. Cannot proceed with model training.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 118\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(scaled_data)\n\u001b[1;32m    119\u001b[0m clusters \u001b[38;5;241m=\u001b[39m trained_model\u001b[38;5;241m.\u001b[39mtransform(scaled_data)\n\u001b[1;32m    121\u001b[0m num_clusters \u001b[38;5;241m=\u001b[39m clusters\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mdistinct()\u001b[38;5;241m.\u001b[39mcount()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pyspark/ml/wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[0;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_java(dataset)\n\u001b[1;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pyspark/ml/wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj\u001b[38;5;241m.\u001b[39mfit(dataset\u001b[38;5;241m.\u001b[39m_jdf)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/py4j/protocol.py:334\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m                 \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 334\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    335\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    336\u001b[0m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name))\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28mtype\u001b[39m \u001b[38;5;241m=\u001b[39m answer[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o12412.fit"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import sum, countDistinct, max, min, datediff, current_date, col, to_date, avg, when, count, mean\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, StringIndexer\n",
    "from pyspark.ml.clustering import KMeans, BisectingKMeans, GaussianMixture\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"Comprehensive Customer Segmentation\").getOrCreate()\n",
    "\n",
    "# Function to load data\n",
    "def read_dataframe_from_sqlite(spark, table_name, db_path, driver):\n",
    "    return spark.read.format(\"jdbc\").option(\"url\", f\"jdbc:sqlite:{db_path}\")\\\n",
    "            .option(\"dbtable\", table_name).option(\"driver\", driver).load()\n",
    "\n",
    "def load_data(spark):\n",
    "    customer_df = read_dataframe_from_sqlite(spark, \"customer_sdf\", \"../data/raw/retail_data.db\",\"org.sqlite.JDBC\")\n",
    "    sales_df = read_dataframe_from_sqlite(spark, \"sales_sdf\", \"../data/raw/retail_data.db\",\"org.sqlite.JDBC\")\n",
    "    product_df = read_dataframe_from_sqlite(spark, \"product_sdf\", \"../data/raw/retail_data.db\",\"org.sqlite.JDBC\")\n",
    "    store_df = read_dataframe_from_sqlite(spark, \"store_sdf\", \"../data/raw/retail_data.db\",\"org.sqlite.JDBC\")\n",
    "    loyalty_df = read_dataframe_from_sqlite(spark, \"loyalty_sdf\", \"../data/raw/retail_data.db\",\"org.sqlite.JDBC\")\n",
    "    feedback_df = read_dataframe_from_sqlite(spark, \"feedback_sdf\", \"../data/raw/retail_data.db\",\"org.sqlite.JDBC\")\n",
    "    return customer_df, sales_df, product_df, store_df, loyalty_df, feedback_df\n",
    "\n",
    "# Calculate RFM metrics with Time\n",
    "def calculate_rfm_with_time(sales_df):\n",
    "    sales_df = sales_df.withColumn(\"Date\", to_date(col(\"Date\"), \"yyyy-MM-dd\"))\n",
    "    rfm_metrics = sales_df.groupBy(\"Customer_ID\").agg(\n",
    "        datediff(current_date(), max(\"Date\")).alias(\"Recency\"),  # Calculate recency\n",
    "        countDistinct(\"Transaction_ID\").alias(\"Frequency\"),      # Calculate frequency\n",
    "        sum(\"Sales_Amount\").alias(\"Monetary\"),                   # Calculate monetary value\n",
    "        datediff(current_date(), min(\"Date\")).alias(\"Customer_Age\")  # Calculate customer age/tenure\n",
    "    )\n",
    "    return rfm_metrics\n",
    "\n",
    "# Loyalty Program Engagement Segmentation\n",
    "def loyalty_program_engagement_segmentation(loyalty_df):\n",
    "    loyalty_metrics = loyalty_df.groupBy(\"Customer_ID\", \"Membership_Tier\").agg(\n",
    "        sum(\"Points_Earned\").alias(\"Total_Points_Earned\"),\n",
    "        sum(col(\"Points_Redeemed\").cast(DoubleType())).alias(\"Total_Points_Redeemed\")\n",
    "    )\n",
    "\n",
    "    # Verify that the Membership_Tier column exists\n",
    "    if \"Membership_Tier\" not in loyalty_metrics.columns:\n",
    "        raise ValueError(\"The column 'Membership_Tier' does not exist in the DataFrame.\")\n",
    "    \n",
    "    # Include Membership_Tier as a feature\n",
    "    indexer = StringIndexer(inputCol=\"Membership_Tier\", outputCol=\"Membership_Tier_Index\")\n",
    "    loyalty_metrics = indexer.fit(loyalty_metrics).transform(loyalty_metrics)\n",
    "    return loyalty_metrics\n",
    "\n",
    "# Customer Lifecycle Segmentation\n",
    "def customer_lifecycle_segmentation(rfm_df):\n",
    "    rfm_df = rfm_df.withColumn(\n",
    "        \"Lifecycle_Segment\",\n",
    "        when(col(\"Recency\") <= 30, \"Active\").when(col(\"Recency\") <= 90, \"Warm\").otherwise(\"Inactive\")\n",
    "    )\n",
    "    indexer = StringIndexer(inputCol=\"Lifecycle_Segment\", outputCol=\"Lifecycle_Segment_Index\")\n",
    "    rfm_df = indexer.fit(rfm_df).transform(rfm_df)\n",
    "    return rfm_df\n",
    "\n",
    "# Product Affinity Segmentation\n",
    "def product_affinity_segmentation(sales_df, product_df):\n",
    "    product_affinity_df = sales_df.join(product_df, \"Product_ID\").groupBy(\"Customer_ID\", \"Category\").agg(\n",
    "        countDistinct(\"Transaction_ID\").alias(\"Category_Purchase_Count\"),\n",
    "        sum(\"Sales_Amount\").alias(\"Category_Spending\")\n",
    "    )\n",
    "    return product_affinity_df\n",
    "\n",
    "# Store Loyalty Segmentation\n",
    "def store_loyalty_segmentation(sales_df, store_df):\n",
    "    store_loyalty_df = sales_df.join(store_df, sales_df[\"Store_ID\"] == store_df[\"Store_Location\"], \"left\").groupBy(\n",
    "        \"Customer_ID\", \"Store_Type\").agg(\n",
    "        countDistinct(\"Transaction_ID\").alias(\"Store_Visit_Frequency\"),\n",
    "        sum(\"Sales_Amount\").alias(\"Store_Total_Spending\")\n",
    "    )\n",
    "    return store_loyalty_df\n",
    "\n",
    "# Feedback Sentiment Segmentation\n",
    "def feedback_sentiment_segmentation(feedback_df):\n",
    "    feedback_sentiment_df = feedback_df.groupBy(\"Customer_ID\").agg(\n",
    "        avg(\"Feedback_Rating\").alias(\"Avg_Sentiment_Score\"),\n",
    "        count(\"Feedback_ID\").alias(\"Total_Feedbacks\")\n",
    "    )\n",
    "    return feedback_sentiment_df\n",
    "\n",
    "# Preprocess data (assemble and scale features)\n",
    "def preprocess_data(customer_df, feature_columns):\n",
    "    for feature in feature_columns:\n",
    "        customer_df = customer_df.withColumn(feature, col(feature).cast(DoubleType()))\n",
    "    \n",
    "    if customer_df.count() == 0:\n",
    "        raise ValueError(\"The DataFrame is empty. Cannot proceed with preprocessing.\")\n",
    "    \n",
    "    mean_values = {}\n",
    "    for feature in feature_columns:\n",
    "        mean_value = customer_df.select(mean(feature)).first()[0]\n",
    "        if mean_value is None:\n",
    "            mean_value = 0.0  # Default to 0.0 if the mean is None\n",
    "        mean_values[feature] = mean_value\n",
    "\n",
    "    customer_df = customer_df.fillna(mean_values)\n",
    "    assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "    feature_vector = assembler.transform(customer_df)\n",
    "    \n",
    "    if feature_vector.select(\"features\").head()[0].size == 0:\n",
    "        raise ValueError(\"Feature vector is empty. Cannot proceed with scaling.\")\n",
    "    \n",
    "    scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withMean=True, withStd=True)\n",
    "    scaled_data = scaler.fit(feature_vector).transform(feature_vector)\n",
    "    return scaled_data\n",
    "\n",
    "# Train and evaluate clustering model\n",
    "def train_and_evaluate_model(model, scaled_data, model_name, feature_columns):\n",
    "    if scaled_data.count() == 0:\n",
    "        raise ValueError(f\"The DataFrame for {model_name} is empty. Cannot proceed with model training.\")\n",
    "    \n",
    "    trained_model = model.fit(scaled_data)\n",
    "    clusters = trained_model.transform(scaled_data)\n",
    "    \n",
    "    num_clusters = clusters.select(\"prediction\").distinct().count()\n",
    "    if num_clusters <= 1:\n",
    "        print(f\"{model_name}: Number of clusters is {num_clusters}. Cannot compute silhouette score.\")\n",
    "        return None, clusters\n",
    "    \n",
    "    evaluator = ClusteringEvaluator(featuresCol=\"scaled_features\", metricName=\"silhouette\", distanceMeasure=\"squaredEuclidean\")\n",
    "    silhouette = evaluator.evaluate(clusters)\n",
    "    \n",
    "    print(f\"{model_name} Silhouette Score: {silhouette}\")\n",
    "    \n",
    "    agg_exprs = [avg(col(feature)).alias(f\"Avg_{feature}\") for feature in feature_columns]\n",
    "    cluster_summary = clusters.groupBy(\"prediction\").agg(*agg_exprs)\n",
    "    return silhouette, clusters, cluster_summary\n",
    "\n",
    "# Load data\n",
    "customer_df, sales_df, product_df, store_df, loyalty_df, feedback_df = load_data(spark)\n",
    "\n",
    "\n",
    "# RFM Segmentation Enhanced with Time\n",
    "rfm_df = calculate_rfm_with_time(sales_df)\n",
    "customer_rfm_df = customer_df.join(rfm_df, \"Customer_ID\", \"left\")\n",
    "scaled_data_rfm = preprocess_data(customer_rfm_df, [\"Recency\", \"Frequency\", \"Monetary\", \"Customer_Age\"])\n",
    "\n",
    "# Loyalty Program Engagement Segmentation\n",
    "loyalty_metrics_df = loyalty_program_engagement_segmentation(loyalty_df)\n",
    "scaled_data_loyalty = preprocess_data(loyalty_metrics_df, [\"Total_Points_Earned\", \"Total_Points_Redeemed\", \"Membership_Tier_Index\"])\n",
    "\n",
    "# Customer Lifecycle Segmentation\n",
    "lifecycle_df = customer_lifecycle_segmentation(rfm_df)\n",
    "scaled_data_lifecycle = preprocess_data(lifecycle_df, [\"Recency\", \"Frequency\", \"Monetary\", \"Lifecycle_Segment_Index\"])\n",
    "\n",
    "# Product Affinity Segmentation\n",
    "product_affinity_df = product_affinity_segmentation(sales_df, product_df)\n",
    "scaled_data_product_affinity = preprocess_data(product_affinity_df, [\"Category_Purchase_Count\", \"Category_Spending\"])\n",
    "\n",
    "# Store Loyalty Segmentation\n",
    "store_loyalty_df = store_loyalty_segmentation(sales_df, store_df)\n",
    "scaled_data_store_loyalty = preprocess_data(store_loyalty_df, [\"Store_Visit_Frequency\", \"Store_Total_Spending\"])\n",
    "\n",
    "# Feedback Sentiment Segmentation\n",
    "feedback_sentiment_df = feedback_sentiment_segmentation(feedback_df)\n",
    "scaled_data_feedback_sentiment = preprocess_data(feedback_sentiment_df, [\"Avg_Sentiment_Score\", \"Total_Feedbacks\"])\n",
    "\n",
    "# Dictionary to store models and their names\n",
    "models = {\n",
    "    \"KMeans\": KMeans(k=4, seed=1, featuresCol=\"scaled_features\"),\n",
    "    \"BisectingKMeans\": BisectingKMeans(k=4, seed=1, featuresCol=\"scaled_features\"),\n",
    "    \"GaussianMixture\": GaussianMixture(k=4, seed=1, featuresCol=\"scaled_features\")\n",
    "}\n",
    "\n",
    "# Dictionary for grid search parameters\n",
    "k_values = [2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "# Train and evaluate each model for each segmentation\n",
    "segmentation_data = {\n",
    "    \"RFM_with_Time\": (scaled_data_rfm, [\"Recency\", \"Frequency\", \"Monetary\", \"Customer_Age\"]),\n",
    "    \"Loyalty_Program_Engagement\": (scaled_data_loyalty, [\"Total_Points_Earned\", \"Total_Points_Redeemed\", \"Membership_Tier_Index\"]),\n",
    "    \"Customer_Lifecycle\": (scaled_data_lifecycle, [\"Recency\", \"Frequency\", \"Monetary\", \"Lifecycle_Segment_Index\"]),\n",
    "    \"Product_Affinity\": (scaled_data_product_affinity, [\"Category_Purchase_Count\", \"Category_Spending\"]),\n",
    "    \"Store_Loyalty\": (scaled_data_store_loyalty, [\"Store_Visit_Frequency\", \"Store_Total_Spending\"]),\n",
    "    \"Feedback_Sentiment\": (scaled_data_feedback_sentiment, [\"Avg_Sentiment_Score\", \"Total_Feedbacks\"])\n",
    "}\n",
    "\n",
    "for seg_name, (data, feature_columns) in segmentation_data.items():\n",
    "    print(f\"Starting segmentation: {seg_name}\")\n",
    "    \n",
    "    best_silhouette = float('-inf')\n",
    "    best_model_name = None\n",
    "    best_clusters = None\n",
    "    best_summary = None\n",
    "\n",
    "    # Grid Search for KMeans\n",
    "    kmeans_results = grid_search_kmeans(data, feature_columns, k_values)\n",
    "    \n",
    "    # Save KMeans results\n",
    "    for k, silhouette, clusters, summary in kmeans_results:\n",
    "        model_name = f\"KMeans_k{k}\"\n",
    "        save_results_to_csv(clusters, summary, seg_name, model_name)\n",
    "        if silhouette > best_silhouette:\n",
    "            best_silhouette = silhouette\n",
    "            best_model_name = model_name\n",
    "            best_clusters = clusters\n",
    "            best_summary = summary\n",
    "    \n",
    "    # Evaluate and save BisectingKMeans and GaussianMixture\n",
    "    for model_name, model in models.items():\n",
    "        silhouette, clusters, summary = train_and_evaluate_model(model, data, f\"{seg_name} - {model_name}\", feature_columns)\n",
    "        if silhouette is not None:\n",
    "            save_results_to_csv(clusters, summary, seg_name, model_name)\n",
    "            if silhouette > best_silhouette:\n",
    "                best_silhouette = silhouette\n",
    "                best_model_name = model_name\n",
    "                best_clusters = clusters\n",
    "                best_summary = summary\n",
    "    \n",
    "    # Save the best model's clusters\n",
    "    if best_clusters is not None:\n",
    "        save_results_to_csv(best_clusters, best_summary, seg_name, best_model_name, is_best=True)\n",
    "\n",
    "# End Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa593c7-f43a-4e0f-9b43-50d0c8d5c9d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.window import Window  # Importing Window\n",
    "import time\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, StringIndexer\n",
    "from pyspark.ml.clustering import KMeans, BisectingKMeans, GaussianMixture\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import os\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"Comprehensive Customer Segmentation\").getOrCreate()\n",
    "\n",
    "def get_timestamp():\n",
    "    return datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    \n",
    "def read_dataframe_from_sqlite(table_name, db_path):\n",
    "    return spark.read.format(\"jdbc\").option(\"url\", f\"jdbc:sqlite:{db_path}\")\\\n",
    "            .option(\"dbtable\", table_name).option(\"driver\", \"org.sqlite.JDBC\").load()\n",
    "\n",
    "def write_df_to_sqlite(df, table_name, db_path):\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    df.to_sql(table_name, conn, if_exists='append', index=False)\n",
    "    conn.close()\n",
    "\n",
    "def get_next_version_number(model_name, db_path):\n",
    "    \"\"\"Retrieve and increment the version number for a given model from the database.\"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT MAX(Version) FROM model_info WHERE Model_Name = ?\", (model_name,))\n",
    "    result = cursor.fetchone()\n",
    "    current_version = result[0] if result[0] is not None else 0\n",
    "    next_version = current_version + 1\n",
    "    conn.close()\n",
    "    return next_version\n",
    "\n",
    "def save_spark_model(model, model_name, db_path):\n",
    "    version = get_next_version_number(model_name, db_path)\n",
    "    date_path = datetime.now().strftime(\"%Y%m%d\")\n",
    "    base_path = f\"./results/models/{date_path}\"\n",
    "    os.makedirs(base_path, exist_ok=True)\n",
    "    file_path = f\"{base_path}/{model_name}_v{version}\"\n",
    "    model.save(file_path)\n",
    "    return file_path, version\n",
    "    \n",
    "def save_model_info_to_db(model_name, file_path, db_path, silhouette_score, version, training_time):\n",
    "    timestamp = datetime.now()\n",
    "    model_info_df = pd.DataFrame([(model_name, file_path, timestamp, silhouette_score, version, training_time)],\n",
    "                                 columns=[\"Model_Name\", \"File_Path\", \"Saved_At\", \"Silhouette_Score\", \"Version\", \"Training_Time\"])\n",
    "    write_df_to_sqlite(model_info_df, \"model_info\", db_path)\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import when, col, lit\n",
    "\n",
    "def preprocess_and_cluster(data, feature_columns, k_values, model_types, segmentation_name,db_path):\n",
    "    if data.rdd.isEmpty():\n",
    "        print(f\"No data available for segmentation: {segmentation_name}\")\n",
    "        return None  # Return None or an empty DataFrame placeholder if needed\n",
    "    \n",
    "    assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "    data = assembler.transform(data.na.fill(0))  # Example: Fill NA with 0 or handle appropriately\n",
    "\n",
    "    if data.select(feature_columns).dropna().rdd.isEmpty():\n",
    "        print(f\"All feature data are null for {segmentation_name}\")\n",
    "        return None\n",
    "\n",
    "    scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
    "    try:\n",
    "        model = scaler.fit(data)\n",
    "        data = model.transform(data)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to scale data for {segmentation_name}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "    best_score = -1\n",
    "    best_model_details = None\n",
    "    for k in k_values:\n",
    "        for model_type in model_types:\n",
    "            model = model_types[model_type](k=k, featuresCol=\"scaled_features\")\n",
    "            try:\n",
    "                start_time = time.time()\n",
    "                trained_model = model.fit(data)\n",
    "                predictions = trained_model.transform(data)\n",
    "                training_time = time.time() - start_time \n",
    "            except Exception as e:\n",
    "                print(f\"Failed to fit or transform model {model_type} for {segmentation_name} with k={k}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "            evaluator = ClusteringEvaluator(featuresCol=\"scaled_features\", metricName=\"silhouette\")\n",
    "            silhouette = evaluator.evaluate(predictions)\n",
    "            if silhouette > best_score:\n",
    "                best_score = silhouette\n",
    "                file_path, version = save_spark_model(trained_model, f\"{segmentation_name}-{model_type}-k{k}\", db_path)\n",
    "                best_model_details = (model_type, k, file_path, silhouette, predictions,training_time, version)\n",
    "    \n",
    "    if best_model_details:\n",
    "        model_type, k, file_path, silhouette, best_predictions, training_time, version = best_model_details\n",
    "        save_model_info_to_db(f\"{segmentation_name}-{model_type}-k{k}\", file_path, db_path, silhouette, version, training_time)\n",
    "        best_predictions = best_predictions.withColumnRenamed(\"prediction\", \"cluster\")\n",
    "        return best_predictions.withColumn(\"Timestamp\", current_timestamp())\n",
    "\n",
    "    return None\n",
    "    \n",
    "\n",
    "def load_data():\n",
    "    db_path = \"../data/raw/retail_data.db\"\n",
    "    return (\n",
    "        read_dataframe_from_sqlite(\"customer_sdf\", db_path),\n",
    "        read_dataframe_from_sqlite(\"sales_sdf\", db_path),\n",
    "        read_dataframe_from_sqlite(\"product_sdf\", db_path),\n",
    "        read_dataframe_from_sqlite(\"store_sdf\", db_path),\n",
    "        read_dataframe_from_sqlite(\"loyalty_sdf\", db_path),\n",
    "        read_dataframe_from_sqlite(\"feedback_sdf\", db_path)\n",
    "    )\n",
    "\n",
    "def calculate_rfm_with_time(sales_df):\n",
    "    # Ensure Date is properly formatted and retained before aggregation\n",
    "    sales_df = sales_df.withColumn(\"Date\", to_date(col(\"Date\"), \"yyyy-MM-dd\"))\n",
    "    \n",
    "    # Window specification to calculate the earliest date for each customer\n",
    "    windowSpec = Window.partitionBy(\"Customer_ID\")\n",
    "    \n",
    "    # Calculate maximum purchase date, frequency, and monetary while retaining Date for further operations\n",
    "    rfm_df = sales_df.groupBy(\"Customer_ID\").agg(\n",
    "        max(\"Date\").alias(\"Last_Purchase_Date\"),\n",
    "        min(\"Date\").alias(\"First_Purchase_Date\"),\n",
    "        countDistinct(\"Transaction_ID\").alias(\"Frequency\"),\n",
    "        sum(\"Sales_Amount\").alias(\"Monetary\")\n",
    "    )\n",
    "\n",
    "    # Calculate Recency and Customer Age\n",
    "    rfm_df = rfm_df.withColumn(\"Recency\", datediff(current_date(), col(\"Last_Purchase_Date\")))\n",
    "    rfm_df = rfm_df.withColumn(\"Customer_Age\", datediff(current_date(), col(\"First_Purchase_Date\")))\n",
    "    \n",
    "    return rfm_df\n",
    "\n",
    "# Add other segmentation functions here\n",
    "def loyalty_program_engagement_segmentation(loyalty_df):\n",
    "    # Assuming Loyalty dataframe has necessary fields\n",
    "    indexer = StringIndexer(inputCol=\"Membership_Tier\", outputCol=\"Membership_Tier_Index\")\n",
    "    loyalty_df = loyalty_df.withColumn(\"Points_Redeemed\", col(\"Points_Redeemed\").cast(DoubleType()))\n",
    "    loyalty_df = loyalty_df.groupBy(\"Customer_ID\").agg(\n",
    "        sum(\"Points_Earned\").alias(\"Total_Points_Earned\"),\n",
    "        sum(\"Points_Redeemed\").alias(\"Total_Points_Redeemed\"),\n",
    "        first(\"Membership_Tier\").alias(\"Membership_Tier\")\n",
    "    )\n",
    "    return indexer.fit(loyalty_df).transform(loyalty_df)\n",
    "\n",
    "def customer_lifecycle_segmentation(rfm_df):\n",
    "    # Adding a lifecycle segment based on recency\n",
    "    lifecycle_df = rfm_df.withColumn(\n",
    "        \"Lifecycle_Segment\",\n",
    "        when(col(\"Recency\") <= 30, \"Active\")\n",
    "        .when((col(\"Recency\") > 30) & (col(\"Recency\") <= 90), \"Warm\")\n",
    "        .otherwise(\"Inactive\")\n",
    "    )\n",
    "    # Convert categorical column to numeric using StringIndexer\n",
    "    indexer = StringIndexer(inputCol=\"Lifecycle_Segment\", outputCol=\"Lifecycle_Segment_Index\")\n",
    "    lifecycle_df = indexer.fit(lifecycle_df).transform(lifecycle_df)\n",
    "    return lifecycle_df\n",
    "\n",
    "\n",
    "def product_affinity_segmentation(sales_df, product_df):\n",
    "    # Assuming sales_df and product_df have been joined and necessary aggregations have been made\n",
    "    return sales_df.join(product_df, \"Product_ID\").groupBy(\"Customer_ID\", \"Category\").agg(\n",
    "        countDistinct(\"Transaction_ID\").alias(\"Category_Purchase_Count\"),\n",
    "        sum(\"Sales_Amount\").alias(\"Category_Spending\")\n",
    "    )\n",
    "\n",
    "def store_loyalty_segmentation(sales_df, store_df):\n",
    "    # Assuming necessary data is available in store_df and has been appropriately joined\n",
    "    return sales_df.join(store_df, \"Store_ID\").groupBy(\n",
    "        \"Customer_ID\", \"Store_Type\").agg(\n",
    "        countDistinct(\"Transaction_ID\").alias(\"Store_Visit_Frequency\"),\n",
    "        sum(\"Sales_Amount\").alias(\"Store_Total_Spending\")\n",
    "    )\n",
    "\n",
    "def feedback_sentiment_segmentation(feedback_df):\n",
    "    # Assume feedback_df contains 'Customer_ID' and 'Feedback_Rating'\n",
    "    return feedback_df.groupBy(\"Customer_ID\").agg(\n",
    "        avg(\"Feedback_Rating\").alias(\"Avg_Sentiment_Score\")\n",
    "    )\n",
    "    \n",
    "def main():\n",
    "    customer_df, sales_df, product_df, store_df, loyalty_df, feedback_df = load_data()\n",
    "\n",
    "    segmentations = {\n",
    "        \"RFM_with_Time\": {\n",
    "        \"data_func\": lambda: calculate_rfm_with_time(sales_df),\n",
    "        \"features\": [\"Recency\", \"Frequency\", \"Monetary\", \"Customer_Age\"]\n",
    "        },\n",
    "        \"Loyalty_Program_Engagement\": {\n",
    "            \"data_func\": lambda: loyalty_program_engagement_segmentation(loyalty_df),\n",
    "            \"features\": [\"Total_Points_Earned\", \"Total_Points_Redeemed\", \"Membership_Tier_Index\"]\n",
    "        },\n",
    "        \"Customer_Lifecycle\": {\n",
    "            \"data_func\": lambda: customer_lifecycle_segmentation(calculate_rfm_with_time(sales_df)),\n",
    "            \"features\": [\"Recency\", \"Lifecycle_Segment_Index\"]\n",
    "        },\n",
    "        \"Product_Affinity\": {\n",
    "            \"data_func\": lambda: product_affinity_segmentation(sales_df, product_df),\n",
    "            \"features\": [\"Category_Purchase_Count\", \"Category_Spending\"]\n",
    "        },\n",
    "        \"Store_Loyalty\": {\n",
    "            \"data_func\": lambda: store_loyalty_segmentation(sales_df, store_df),\n",
    "            \"features\": [\"Store_Visit_Frequency\", \"Store_Total_Spending\"]\n",
    "        },\n",
    "        \"Feedback_Sentiment\": {\n",
    "            \"data_func\": lambda: feedback_sentiment_segmentation(feedback_df),\n",
    "            \"features\": [\"Avg_Sentiment_Score\"]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    \n",
    "    model_types = {\n",
    "        \"KMeans\": KMeans,\n",
    "        \"BisectingKMeans\": BisectingKMeans,\n",
    "        \"GaussianMixture\": GaussianMixture\n",
    "    }\n",
    "    k_values = [2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "    for seg_name, seg_info in segmentations.items():\n",
    "        seg_data = seg_info[\"data_func\"]()\n",
    "        predictions = preprocess_and_cluster(seg_data, seg_info[\"features\"], k_values, model_types, seg_name,\"../data/raw/retail_data.db\")\n",
    "        if predictions:\n",
    "            predictions_pandas = predictions.select(\"Customer_ID\", \"cluster\", \"Timestamp\").toPandas()\n",
    "            write_df_to_sqlite(predictions_pandas, f\"{seg_name}_results\", \"../data/raw/retail_data.db\")\n",
    "\n",
    "    spark.stop()\n",
    "\n",
    "def create_or_update_table(db_path):\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    # This will create the table if it does not exist with the appropriate columns\n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS model_info (\n",
    "            Model_Name TEXT,\n",
    "            File_Path TEXT,\n",
    "            Saved_At TIMESTAMP,\n",
    "            Silhouette_Score FLOAT,\n",
    "            Version INTEGER,\n",
    "            Training_Time FLOAT\n",
    "        );\n",
    "    ''')\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def write_df_to_sqlite(df, table_name, db_path):\n",
    "    create_or_update_table(db_path)  # Ensure the table exists with correct schema\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    df.to_sql(table_name, conn, if_exists='append', index=False)\n",
    "    conn.close()\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    create_or_update_table(\"../data/raw/retail_data.db\")  # Ensure DB schema is correct\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf2b5fd3-0022-4fcc-ae88-f7cb15b13073",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/15 23:21:25 WARN Utils: Your hostname, Aadarshs-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 10.0.0.50 instead (on interface en0)\n",
      "24/08/15 23:21:25 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "24/08/15 23:21:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2024-08-15 23:21:30,311 - INFO - Database table 'model_info' checked or updated.\n",
      "2024-08-15 23:21:30,311 - INFO - Reading DataFrame from SQLite: Table=customer_sdf, DB=../data/raw/retail_data.db\n",
      "2024-08-15 23:21:33,897 - INFO - Reading DataFrame from SQLite: Table=sales_sdf, DB=../data/raw/retail_data.db\n",
      "2024-08-15 23:21:33,947 - INFO - Reading DataFrame from SQLite: Table=product_sdf, DB=../data/raw/retail_data.db\n",
      "2024-08-15 23:21:34,006 - INFO - Reading DataFrame from SQLite: Table=store_sdf, DB=../data/raw/retail_data.db\n",
      "2024-08-15 23:21:34,046 - INFO - Reading DataFrame from SQLite: Table=loyalty_sdf, DB=../data/raw/retail_data.db\n",
      "2024-08-15 23:21:34,086 - INFO - Reading DataFrame from SQLite: Table=feedback_sdf, DB=../data/raw/retail_data.db\n",
      "2024-08-15 23:21:34,124 - INFO - Processing segmentation: RFM_with_Time\n",
      "2024-08-15 23:21:34,535 - INFO - Starting preprocessing and clustering for RFM_with_Time\n",
      "2024-08-15 23:22:00,795 - INFO - Training KMeans with k=2 for RFM_with_Time     \n",
      "2024-08-15 23:22:27,145 - INFO - Saving Spark model: RFM_with_Time-KMeans-k2    \n",
      "2024-08-15 23:22:27,147 - INFO - Fetching next version number for model: RFM_with_Time-KMeans-k2\n",
      "2024-08-15 23:22:30,996 - INFO - Model saved at ./results/models/20240815/RFM_with_Time-KMeans-k2_v1 with version 1\n",
      "2024-08-15 23:22:31,018 - INFO - Training BisectingKMeans with k=2 for RFM_with_Time\n",
      "2024-08-15 23:22:46,533 - INFO - Training GaussianMixture with k=2 for RFM_with_Time\n",
      "2024-08-15 23:23:02,292 - INFO - Training KMeans with k=3 for RFM_with_Time     \n",
      "2024-08-15 23:23:14,140 - INFO - Training BisectingKMeans with k=3 for RFM_with_Time\n",
      "2024-08-15 23:23:27,754 - INFO - Training GaussianMixture with k=3 for RFM_with_Time\n",
      "2024-08-15 23:23:40,906 - INFO - Training KMeans with k=4 for RFM_with_Time     \n",
      "2024-08-15 23:23:51,845 - INFO - Saving Spark model: RFM_with_Time-KMeans-k4    \n",
      "2024-08-15 23:23:51,846 - INFO - Fetching next version number for model: RFM_with_Time-KMeans-k4\n",
      "2024-08-15 23:23:53,218 - INFO - Model saved at ./results/models/20240815/RFM_with_Time-KMeans-k4_v1 with version 1\n",
      "2024-08-15 23:23:53,239 - INFO - Training BisectingKMeans with k=4 for RFM_with_Time\n",
      "2024-08-15 23:24:07,209 - INFO - Training GaussianMixture with k=4 for RFM_with_Time\n",
      "2024-08-15 23:24:24,664 - INFO - Training KMeans with k=5 for RFM_with_Time     \n",
      "2024-08-15 23:24:36,990 - INFO - Training BisectingKMeans with k=5 for RFM_with_Time\n",
      "2024-08-15 23:24:48,571 - INFO - Training GaussianMixture with k=5 for RFM_with_Time\n",
      "2024-08-15 23:25:06,023 - INFO - Training KMeans with k=6 for RFM_with_Time     \n",
      "2024-08-15 23:25:15,792 - INFO - Training BisectingKMeans with k=6 for RFM_with_Time\n",
      "2024-08-15 23:25:29,108 - INFO - Training GaussianMixture with k=6 for RFM_with_Time\n",
      "2024-08-15 23:25:47,460 - INFO - Training KMeans with k=7 for RFM_with_Time     \n",
      "2024-08-15 23:25:57,830 - INFO - Training BisectingKMeans with k=7 for RFM_with_Time\n",
      "2024-08-15 23:26:07,835 - INFO - Training GaussianMixture with k=7 for RFM_with_Time\n",
      "2024-08-15 23:26:25,153 - INFO - Training KMeans with k=8 for RFM_with_Time     \n",
      "2024-08-15 23:26:38,135 - INFO - Training BisectingKMeans with k=8 for RFM_with_Time\n",
      "2024-08-15 23:26:48,211 - INFO - Training GaussianMixture with k=8 for RFM_with_Time\n",
      "2024-08-15 23:27:05,826 - INFO - Training KMeans with k=9 for RFM_with_Time     \n",
      "2024-08-15 23:27:20,201 - INFO - Training BisectingKMeans with k=9 for RFM_with_Time\n",
      "2024-08-15 23:27:34,384 - INFO - Training GaussianMixture with k=9 for RFM_with_Time\n",
      "2024-08-15 23:27:55,890 - INFO - Training KMeans with k=10 for RFM_with_Time    \n",
      "2024-08-15 23:28:06,393 - INFO - Training BisectingKMeans with k=10 for RFM_with_Time\n",
      "2024-08-15 23:28:18,279 - INFO - Training GaussianMixture with k=10 for RFM_with_Time\n",
      "2024-08-15 23:28:39,098 - INFO - Saving model info to database for RFM_with_Time-KMeans-k4, version 1\n",
      "2024-08-15 23:28:39,122 - INFO - Writing DataFrame to SQLite: Table=model_info, DB=../data/raw/retail_data.db\n",
      "2024-08-15 23:28:41,904 - INFO - Writing DataFrame to SQLite: Table=RFM_with_Time_results, DB=../data/raw/retail_data.db\n",
      "2024-08-15 23:28:41,941 - INFO - Processing segmentation: Loyalty_Program_Engagement\n",
      "2024-08-15 23:28:44,304 - INFO - Starting preprocessing and clustering for Loyalty_Program_Engagement\n",
      "2024-08-15 23:28:47,074 - INFO - Training KMeans with k=2 for Loyalty_Program_Engagement\n",
      "2024-08-15 23:28:52,443 - INFO - Saving Spark model: Loyalty_Program_Engagement-KMeans-k2\n",
      "2024-08-15 23:28:52,444 - INFO - Fetching next version number for model: Loyalty_Program_Engagement-KMeans-k2\n",
      "2024-08-15 23:28:53,452 - INFO - Model saved at ./results/models/20240815/Loyalty_Program_Engagement-KMeans-k2_v1 with version 1\n",
      "2024-08-15 23:28:53,467 - INFO - Training BisectingKMeans with k=2 for Loyalty_Program_Engagement\n",
      "2024-08-15 23:28:57,426 - INFO - Training GaussianMixture with k=2 for Loyalty_Program_Engagement\n",
      "2024-08-15 23:29:01,265 - INFO - Training KMeans with k=3 for Loyalty_Program_Engagement\n",
      "2024-08-15 23:29:04,777 - INFO - Training BisectingKMeans with k=3 for Loyalty_Program_Engagement\n",
      "2024-08-15 23:29:09,148 - INFO - Training GaussianMixture with k=3 for Loyalty_Program_Engagement\n",
      "2024-08-15 23:29:17,453 - INFO - Training KMeans with k=4 for Loyalty_Program_Engagement\n",
      "2024-08-15 23:29:21,621 - INFO - Training BisectingKMeans with k=4 for Loyalty_Program_Engagement\n",
      "2024-08-15 23:29:25,559 - INFO - Training GaussianMixture with k=4 for Loyalty_Program_Engagement\n",
      "2024-08-15 23:29:32,679 - INFO - Training KMeans with k=5 for Loyalty_Program_Engagement\n",
      "2024-08-15 23:29:37,117 - INFO - Training BisectingKMeans with k=5 for Loyalty_Program_Engagement\n",
      "2024-08-15 23:29:42,377 - INFO - Training GaussianMixture with k=5 for Loyalty_Program_Engagement\n",
      "2024-08-15 23:29:50,530 - INFO - Training KMeans with k=6 for Loyalty_Program_Engagement\n",
      "2024-08-15 23:29:53,943 - INFO - Training BisectingKMeans with k=6 for Loyalty_Program_Engagement\n",
      "2024-08-15 23:29:59,032 - INFO - Training GaussianMixture with k=6 for Loyalty_Program_Engagement\n",
      "2024-08-15 23:30:14,196 - INFO - Training KMeans with k=7 for Loyalty_Program_Engagement\n",
      "2024-08-15 23:30:17,561 - INFO - Training BisectingKMeans with k=7 for Loyalty_Program_Engagement\n",
      "2024-08-15 23:30:23,689 - INFO - Training GaussianMixture with k=7 for Loyalty_Program_Engagement\n",
      "2024-08-15 23:30:37,942 - INFO - Training KMeans with k=8 for Loyalty_Program_Engagement\n",
      "2024-08-15 23:30:41,856 - INFO - Training BisectingKMeans with k=8 for Loyalty_Program_Engagement\n",
      "2024-08-15 23:30:47,711 - INFO - Training GaussianMixture with k=8 for Loyalty_Program_Engagement\n",
      "2024-08-15 23:31:06,070 - INFO - Training KMeans with k=9 for Loyalty_Program_Engagement\n",
      "2024-08-15 23:31:09,394 - INFO - Training BisectingKMeans with k=9 for Loyalty_Program_Engagement\n",
      "2024-08-15 23:31:15,474 - INFO - Training GaussianMixture with k=9 for Loyalty_Program_Engagement\n",
      "2024-08-15 23:31:35,128 - INFO - Training KMeans with k=10 for Loyalty_Program_Engagement\n",
      "2024-08-15 23:31:38,416 - INFO - Training BisectingKMeans with k=10 for Loyalty_Program_Engagement\n",
      "2024-08-15 23:31:45,989 - INFO - Training GaussianMixture with k=10 for Loyalty_Program_Engagement\n",
      "2024-08-15 23:32:07,186 - INFO - Saving model info to database for Loyalty_Program_Engagement-KMeans-k2, version 1\n",
      "2024-08-15 23:32:07,191 - INFO - Writing DataFrame to SQLite: Table=model_info, DB=../data/raw/retail_data.db\n",
      "2024-08-15 23:32:08,092 - INFO - Writing DataFrame to SQLite: Table=Loyalty_Program_Engagement_results, DB=../data/raw/retail_data.db\n",
      "2024-08-15 23:32:08,136 - INFO - Processing segmentation: Customer_Lifecycle\n",
      "2024-08-15 23:32:09,528 - INFO - Starting preprocessing and clustering for Customer_Lifecycle\n",
      "2024-08-15 23:32:17,824 - INFO - Training KMeans with k=2 for Customer_Lifecycle\n",
      "2024-08-15 23:32:25,746 - INFO - Saving Spark model: Customer_Lifecycle-KMeans-k2\n",
      "2024-08-15 23:32:25,749 - INFO - Fetching next version number for model: Customer_Lifecycle-KMeans-k2\n",
      "2024-08-15 23:32:26,750 - INFO - Model saved at ./results/models/20240815/Customer_Lifecycle-KMeans-k2_v1 with version 1\n",
      "2024-08-15 23:32:26,763 - INFO - Training BisectingKMeans with k=2 for Customer_Lifecycle\n",
      "2024-08-15 23:32:32,870 - INFO - Training GaussianMixture with k=2 for Customer_Lifecycle\n",
      "2024-08-15 23:32:40,332 - INFO - Training KMeans with k=3 for Customer_Lifecycle\n",
      "2024-08-15 23:32:45,170 - INFO - Training BisectingKMeans with k=3 for Customer_Lifecycle\n",
      "2024-08-15 23:32:50,880 - INFO - Training GaussianMixture with k=3 for Customer_Lifecycle\n",
      "2024-08-15 23:33:02,077 - INFO - Training KMeans with k=4 for Customer_Lifecycle\n",
      "2024-08-15 23:33:07,226 - INFO - Training BisectingKMeans with k=4 for Customer_Lifecycle\n",
      "2024-08-15 23:33:12,824 - INFO - Training GaussianMixture with k=4 for Customer_Lifecycle\n",
      "2024-08-15 23:33:25,795 - INFO - Training KMeans with k=5 for Customer_Lifecycle\n",
      "2024-08-15 23:33:30,188 - INFO - Training BisectingKMeans with k=5 for Customer_Lifecycle\n",
      "2024-08-15 23:33:36,054 - INFO - Training GaussianMixture with k=5 for Customer_Lifecycle\n",
      "2024-08-15 23:33:50,391 - INFO - Training KMeans with k=6 for Customer_Lifecycle\n",
      "2024-08-15 23:33:55,306 - INFO - Training BisectingKMeans with k=6 for Customer_Lifecycle\n",
      "2024-08-15 23:34:01,892 - INFO - Training GaussianMixture with k=6 for Customer_Lifecycle\n",
      "2024-08-15 23:34:17,459 - INFO - Training KMeans with k=7 for Customer_Lifecycle\n",
      "2024-08-15 23:34:22,038 - INFO - Training BisectingKMeans with k=7 for Customer_Lifecycle\n",
      "2024-08-15 23:34:29,033 - INFO - Training GaussianMixture with k=7 for Customer_Lifecycle\n",
      "2024-08-15 23:34:46,744 - INFO - Training KMeans with k=8 for Customer_Lifecycle\n",
      "2024-08-15 23:34:51,467 - INFO - Training BisectingKMeans with k=8 for Customer_Lifecycle\n",
      "2024-08-15 23:34:57,592 - INFO - Training GaussianMixture with k=8 for Customer_Lifecycle\n",
      "2024-08-15 23:35:16,523 - INFO - Training KMeans with k=9 for Customer_Lifecycle\n",
      "2024-08-15 23:35:21,220 - INFO - Training BisectingKMeans with k=9 for Customer_Lifecycle\n",
      "2024-08-15 23:35:28,907 - INFO - Training GaussianMixture with k=9 for Customer_Lifecycle\n",
      "2024-08-15 23:35:49,126 - INFO - Training KMeans with k=10 for Customer_Lifecycle\n",
      "2024-08-15 23:35:53,715 - INFO - Training BisectingKMeans with k=10 for Customer_Lifecycle\n",
      "2024-08-15 23:36:01,393 - INFO - Training GaussianMixture with k=10 for Customer_Lifecycle\n",
      "2024-08-15 23:36:22,364 - INFO - Saving model info to database for Customer_Lifecycle-KMeans-k2, version 1\n",
      "2024-08-15 23:36:22,370 - INFO - Writing DataFrame to SQLite: Table=model_info, DB=../data/raw/retail_data.db\n",
      "2024-08-15 23:36:23,863 - INFO - Writing DataFrame to SQLite: Table=Customer_Lifecycle_results, DB=../data/raw/retail_data.db\n",
      "2024-08-15 23:36:23,919 - INFO - Processing segmentation: Product_Affinity\n",
      "2024-08-15 23:36:24,096 - INFO - Starting preprocessing and clustering for Product_Affinity\n",
      "2024-08-15 23:36:32,994 - INFO - Training KMeans with k=2 for Product_Affinity  \n",
      "2024-08-15 23:36:54,452 - INFO - Saving Spark model: Product_Affinity-KMeans-k2 \n",
      "2024-08-15 23:36:54,453 - INFO - Fetching next version number for model: Product_Affinity-KMeans-k2\n",
      "2024-08-15 23:36:56,342 - INFO - Model saved at ./results/models/20240815/Product_Affinity-KMeans-k2_v1 with version 1\n",
      "2024-08-15 23:36:56,358 - INFO - Training BisectingKMeans with k=2 for Product_Affinity\n",
      "2024-08-15 23:37:12,388 - INFO - Training GaussianMixture with k=2 for Product_Affinity\n",
      "2024-08-15 23:37:28,835 - INFO - Training KMeans with k=3 for Product_Affinity  \n",
      "2024-08-15 23:37:45,269 - INFO - Training BisectingKMeans with k=3 for Product_Affinity\n",
      "2024-08-15 23:37:59,836 - INFO - Training GaussianMixture with k=3 for Product_Affinity\n",
      "2024-08-15 23:38:23,024 - INFO - Training KMeans with k=4 for Product_Affinity  \n",
      "2024-08-15 23:38:34,427 - INFO - Training BisectingKMeans with k=4 for Product_Affinity\n",
      "2024-08-15 23:38:47,346 - INFO - Training GaussianMixture with k=4 for Product_Affinity\n",
      "2024-08-15 23:39:15,839 - INFO - Training KMeans with k=5 for Product_Affinity  \n",
      "2024-08-15 23:39:29,990 - INFO - Training BisectingKMeans with k=5 for Product_Affinity\n",
      "2024-08-15 23:39:45,948 - INFO - Training GaussianMixture with k=5 for Product_Affinity\n",
      "2024-08-15 23:40:15,437 - INFO - Training KMeans with k=6 for Product_Affinity  \n",
      "2024-08-15 23:40:27,249 - INFO - Training BisectingKMeans with k=6 for Product_Affinity\n",
      "2024-08-15 23:40:41,410 - INFO - Training GaussianMixture with k=6 for Product_Affinity\n",
      "2024-08-15 23:41:16,193 - INFO - Training KMeans with k=7 for Product_Affinity  \n",
      "2024-08-15 23:41:28,014 - INFO - Training BisectingKMeans with k=7 for Product_Affinity\n",
      "2024-08-15 23:41:44,141 - INFO - Training GaussianMixture with k=7 for Product_Affinity\n",
      "2024-08-15 23:42:18,978 - INFO - Training KMeans with k=8 for Product_Affinity  \n",
      "2024-08-15 23:42:32,716 - INFO - Training BisectingKMeans with k=8 for Product_Affinity\n",
      "2024-08-15 23:42:49,873 - INFO - Training GaussianMixture with k=8 for Product_Affinity\n",
      "2024-08-15 23:43:28,744 - INFO - Training KMeans with k=9 for Product_Affinity  \n",
      "2024-08-15 23:43:42,674 - INFO - Training BisectingKMeans with k=9 for Product_Affinity\n",
      "2024-08-15 23:43:59,256 - INFO - Training GaussianMixture with k=9 for Product_Affinity\n",
      "2024-08-15 23:44:34,978 - INFO - Training KMeans with k=10 for Product_Affinity \n",
      "2024-08-15 23:44:46,674 - INFO - Training BisectingKMeans with k=10 for Product_Affinity\n",
      "2024-08-15 23:45:04,636 - INFO - Training GaussianMixture with k=10 for Product_Affinity\n",
      "2024-08-15 23:45:52,356 - INFO - Saving model info to database for Product_Affinity-KMeans-k2, version 1\n",
      "2024-08-15 23:45:52,361 - INFO - Writing DataFrame to SQLite: Table=model_info, DB=../data/raw/retail_data.db\n",
      "2024-08-15 23:45:57,089 - INFO - Writing DataFrame to SQLite: Table=Product_Affinity_results, DB=../data/raw/retail_data.db\n",
      "2024-08-15 23:45:57,250 - INFO - Processing segmentation: Store_Loyalty\n",
      "2024-08-15 23:45:57,417 - INFO - Starting preprocessing and clustering for Store_Loyalty\n",
      "2024-08-15 23:45:58,864 - WARNING - No data available for segmentation: Store_Loyalty\n",
      "2024-08-15 23:45:58,865 - INFO - Processing segmentation: Feedback_Sentiment\n",
      "2024-08-15 23:45:58,938 - INFO - Starting preprocessing and clustering for Feedback_Sentiment\n",
      "2024-08-15 23:46:01,107 - INFO - Training KMeans with k=2 for Feedback_Sentiment\n",
      "2024-08-15 23:46:05,145 - INFO - Saving Spark model: Feedback_Sentiment-KMeans-k2\n",
      "2024-08-15 23:46:05,147 - INFO - Fetching next version number for model: Feedback_Sentiment-KMeans-k2\n",
      "2024-08-15 23:46:05,960 - INFO - Model saved at ./results/models/20240815/Feedback_Sentiment-KMeans-k2_v1 with version 1\n",
      "2024-08-15 23:46:05,975 - INFO - Training BisectingKMeans with k=2 for Feedback_Sentiment\n",
      "2024-08-15 23:46:10,374 - INFO - Training GaussianMixture with k=2 for Feedback_Sentiment\n",
      "2024-08-15 23:46:19,059 - INFO - Training KMeans with k=3 for Feedback_Sentiment\n",
      "2024-08-15 23:46:22,480 - INFO - Saving Spark model: Feedback_Sentiment-KMeans-k3\n",
      "2024-08-15 23:46:22,483 - INFO - Fetching next version number for model: Feedback_Sentiment-KMeans-k3\n",
      "2024-08-15 23:46:23,320 - INFO - Model saved at ./results/models/20240815/Feedback_Sentiment-KMeans-k3_v1 with version 1\n",
      "2024-08-15 23:46:23,339 - INFO - Training BisectingKMeans with k=3 for Feedback_Sentiment\n",
      "2024-08-15 23:46:28,199 - INFO - Training GaussianMixture with k=3 for Feedback_Sentiment\n",
      "2024-08-15 23:46:38,328 - INFO - Saving Spark model: Feedback_Sentiment-GaussianMixture-k3\n",
      "2024-08-15 23:46:38,331 - INFO - Fetching next version number for model: Feedback_Sentiment-GaussianMixture-k3\n",
      "2024-08-15 23:46:39,425 - INFO - Model saved at ./results/models/20240815/Feedback_Sentiment-GaussianMixture-k3_v1 with version 1\n",
      "2024-08-15 23:46:39,432 - INFO - Training KMeans with k=4 for Feedback_Sentiment\n",
      "2024-08-15 23:46:42,777 - INFO - Training BisectingKMeans with k=4 for Feedback_Sentiment\n",
      "2024-08-15 23:46:47,723 - INFO - Training GaussianMixture with k=4 for Feedback_Sentiment\n",
      "2024-08-15 23:46:59,220 - INFO - Training KMeans with k=5 for Feedback_Sentiment\n",
      "2024-08-15 23:47:01,509 - INFO - Training BisectingKMeans with k=5 for Feedback_Sentiment\n",
      "2024-08-15 23:47:06,335 - INFO - Training GaussianMixture with k=5 for Feedback_Sentiment\n",
      "2024-08-15 23:47:18,427 - INFO - Training KMeans with k=6 for Feedback_Sentiment\n",
      "2024-08-15 23:47:21,404 - INFO - Saving Spark model: Feedback_Sentiment-KMeans-k6\n",
      "2024-08-15 23:47:21,406 - INFO - Fetching next version number for model: Feedback_Sentiment-KMeans-k6\n",
      "2024-08-15 23:47:22,032 - INFO - Model saved at ./results/models/20240815/Feedback_Sentiment-KMeans-k6_v1 with version 1\n",
      "2024-08-15 23:47:22,037 - INFO - Training BisectingKMeans with k=6 for Feedback_Sentiment\n",
      "2024-08-15 23:47:27,251 - INFO - Training GaussianMixture with k=6 for Feedback_Sentiment\n",
      "2024-08-15 23:47:41,453 - INFO - Training KMeans with k=7 for Feedback_Sentiment\n",
      "2024-08-15 23:47:43,894 - INFO - Training BisectingKMeans with k=7 for Feedback_Sentiment\n",
      "2024-08-15 23:47:48,979 - INFO - Training GaussianMixture with k=7 for Feedback_Sentiment\n",
      "2024-08-15 23:48:04,219 - INFO - Training KMeans with k=8 for Feedback_Sentiment\n",
      "2024-08-15 23:48:07,302 - INFO - Training BisectingKMeans with k=8 for Feedback_Sentiment\n",
      "2024-08-15 23:48:12,206 - INFO - Training GaussianMixture with k=8 for Feedback_Sentiment\n",
      "2024-08-15 23:48:29,193 - INFO - Training KMeans with k=9 for Feedback_Sentiment\n",
      "2024-08-15 23:48:31,955 - INFO - Training BisectingKMeans with k=9 for Feedback_Sentiment\n",
      "2024-08-15 23:48:38,518 - INFO - Training GaussianMixture with k=9 for Feedback_Sentiment\n",
      "2024-08-15 23:48:56,122 - INFO - Training KMeans with k=10 for Feedback_Sentiment\n",
      "2024-08-15 23:48:58,481 - INFO - Training BisectingKMeans with k=10 for Feedback_Sentiment\n",
      "2024-08-15 23:49:04,490 - INFO - Training GaussianMixture with k=10 for Feedback_Sentiment\n",
      "2024-08-15 23:49:24,799 - INFO - Saving model info to database for Feedback_Sentiment-KMeans-k6, version 1\n",
      "2024-08-15 23:49:24,805 - INFO - Writing DataFrame to SQLite: Table=model_info, DB=../data/raw/retail_data.db\n",
      "2024-08-15 23:49:25,945 - INFO - Writing DataFrame to SQLite: Table=Feedback_Sentiment_results, DB=../data/raw/retail_data.db\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1aef639-7688-4cf9-bcb3-cd7c98ceeb38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
