{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf5ab0b-23ea-44a3-bdc2-0ae364e4fee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "notebook_dir = os.getcwd()\n",
    "src_path = os.path.abspath(os.path.join(notebook_dir, '..', 'src'))\n",
    "sys.path.append(src_path)\n",
    "\n",
    "from utils.spark_utils import load_config, initialize_spark, insert_dataframe_to_sqlite, read_dataframe_from_sqlite\n",
    "from pyspark.sql.functions import col, count, avg, sum, format_number,current_date\n",
    "\n",
    "# Load configuration\n",
    "config = load_config(\"../configs/data_config.json\")\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = initialize_spark(config[\"spark\"][\"app_name\"], \"../jars/sqlite-jdbc-3.46.0.1.jar\")\n",
    "\n",
    "product_df = read_dataframe_from_sqlite(spark, \"product_sdf\", \"../data/raw/retail_data.db\",\"org.sqlite.JDBC\")\n",
    "store_df = read_dataframe_from_sqlite(spark, \"store_sdf\", \"../data/raw/retail_data.db\",\"org.sqlite.JDBC\")\n",
    "customer_df = read_dataframe_from_sqlite(spark, \"customer_sdf\", \"../data/raw/retail_data.db\",\"org.sqlite.JDBC\")\n",
    "time_df = read_dataframe_from_sqlite(spark, \"time_sdf\", \"../data/raw/retail_data.db\",\"org.sqlite.JDBC\")\n",
    "sales_df = read_dataframe_from_sqlite(spark, \"sales_sdf\", \"../data/raw/retail_data.db\",\"org.sqlite.JDBC\")\n",
    "supplier_df = read_dataframe_from_sqlite(spark, \"supplier_sdf\", \"../data/raw/retail_data.db\",\"org.sqlite.JDBC\")\n",
    "feedback_df = read_dataframe_from_sqlite(spark, \"feedback_sdf\", \"../data/raw/retail_data.db\",\"org.sqlite.JDBC\")\n",
    "loyalty_df = read_dataframe_from_sqlite(spark, \"loyalty_sdf\", \"../data/raw/retail_data.db\",\"org.sqlite.JDBC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96696f3a-ae20-4059-935c-97845c16edad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print schema for all DataFrames\n",
    "print(\"=== Product DataFrame Schema ===\")\n",
    "product_df.printSchema()\n",
    "\n",
    "print(\"=== Store DataFrame Schema ===\")\n",
    "store_df.printSchema()\n",
    "\n",
    "print(\"=== Customer DataFrame Schema ===\")\n",
    "customer_df.printSchema()\n",
    "\n",
    "print(\"=== Time DataFrame Schema ===\")\n",
    "time_df.printSchema()\n",
    "\n",
    "print(\"=== Sales DataFrame Schema ===\")\n",
    "sales_df.printSchema()\n",
    "\n",
    "print(\"=== Supplier DataFrame Schema ===\")\n",
    "supplier_df.printSchema()\n",
    "\n",
    "print(\"=== Feedback DataFrame Schema ===\")\n",
    "feedback_df.printSchema()\n",
    "\n",
    "print(\"=== Loyalty DataFrame Schema ===\")\n",
    "loyalty_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39ba9a2-2c06-494a-b1f8-65d1ce0aeedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import necessary functions\n",
    "from pyspark.sql.functions import count, avg, sum, col, countDistinct, format_number, year, datediff, current_date, max, min\n",
    "\n",
    "### 1. Product Analysis\n",
    "print(\"=== Product Analysis ===\")\n",
    "product_df.printSchema()\n",
    "\n",
    "# Basic stats\n",
    "total_products = product_df.select(\"Product_ID\").distinct().count()\n",
    "total_brands = product_df.select(\"Brand_Name\").distinct().count()\n",
    "total_categories = product_df.select(\"Category\").distinct().count()\n",
    "total_suppliers = product_df.select(\"Supplier_ID\").distinct().count()\n",
    "\n",
    "print(f\"Total Products: {total_products}\")\n",
    "print(f\"Total Brands: {total_brands}\")\n",
    "print(f\"Total Categories: {total_categories}\")\n",
    "print(f\"Total Suppliers: {total_suppliers}\")\n",
    "\n",
    "# Products and Avg. Price Across Categories\n",
    "print(\"Products and Average Price Across Categories\")\n",
    "category_statistics = product_df.groupBy(\"Category\").agg(\n",
    "    count(\"Product_ID\").alias(\"Product_Count\"),\n",
    "    format_number(avg(\"Price\"), 2).alias(\"Average_Price\")\n",
    ")\n",
    "category_statistics.show()\n",
    "\n",
    "# Brands by Product Count\n",
    "print(\"Brands by Product Count\")\n",
    "brand_statistics = product_df.groupBy(\"Brand_Name\").agg(\n",
    "    count(\"Product_ID\").alias(\"Product_Count\"),\n",
    "    format_number(avg(\"Price\"), 2).alias(\"Average_Price\")\n",
    ")\n",
    "brand_statistics.orderBy(col(\"Product_Count\").desc()).show(10)\n",
    "\n",
    "### 2. Store Analysis\n",
    "print(\"=== Store Analysis ===\")\n",
    "store_df.printSchema()\n",
    "\n",
    "# Basic stats\n",
    "total_stores = store_df.select(\"Store_ID\").distinct().count()\n",
    "total_store_locations = store_df.select(\"Store_Location\").distinct().count()\n",
    "\n",
    "print(f\"Total Stores: {total_stores}\")\n",
    "print(f\"Total Store Locations: {total_store_locations}\")\n",
    "\n",
    "# Stores and Avg. Size by Store Type\n",
    "print(\"Stores and Average Size by Store Type\")\n",
    "store_statistics = store_df.groupBy(\"Store_Type\").agg(\n",
    "    count(\"Store_ID\").alias(\"Store_Count\"),\n",
    "    format_number(avg(\"Store_Size\"), 2).alias(\"Average_Store_Size\")\n",
    ")\n",
    "store_statistics.show()\n",
    "\n",
    "# Store Size Distribution\n",
    "print(\"Store Size Distribution\")\n",
    "store_size_distribution = store_df.groupBy(\"Store_Size\").agg(\n",
    "    count(\"Store_ID\").alias(\"Store_Count\")\n",
    ")\n",
    "store_size_distribution.orderBy(col(\"Store_Size\").desc()).show()\n",
    "\n",
    "### 3. Customer Analysis\n",
    "print(\"=== Customer Analysis ===\")\n",
    "customer_df.printSchema()\n",
    "\n",
    "# Basic stats\n",
    "total_customers = customer_df.select(\"Customer_ID\").distinct().count()\n",
    "print(f\"Total Customers: {total_customers}\")\n",
    "\n",
    "# Customers by State\n",
    "print(\"Customer Distribution by State\")\n",
    "customer_state_stats = customer_df.groupBy(\"State\").agg(\n",
    "    count(\"Customer_ID\").alias(\"Customer_Count\")\n",
    ")\n",
    "customer_state_stats.show()\n",
    "\n",
    "# Gender Distribution\n",
    "print(\"Gender Distribution\")\n",
    "customer_df.groupBy(\"Gender\").count().show()\n",
    "\n",
    "# Age Analysis\n",
    "print(\"Customer Age Analysis\")\n",
    "current_year = year(current_date())\n",
    "customer_age_df = customer_df.withColumn(\"Age\", current_year - year(col(\"DOB\")))\n",
    "\n",
    "# Age Distribution\n",
    "print(\"Age Distribution\")\n",
    "age_distribution = customer_age_df.groupBy(\"Age\").agg(\n",
    "    count(\"Customer_ID\").alias(\"Customer_Count\")\n",
    ")\n",
    "age_distribution.orderBy(col(\"Age\").desc()).show()\n",
    "\n",
    "# Customer Join Date Distribution\n",
    "print(\"Customer Join Date Distribution\")\n",
    "customer_df.groupBy(year(\"Customer_Join_Date\").alias(\"Join_Year\")).count().orderBy(\"Join_Year\").show()\n",
    "\n",
    "### 4. Sales Analysis\n",
    "print(\"=== Sales Analysis ===\")\n",
    "sales_df.printSchema()\n",
    "\n",
    "# Basic stats\n",
    "total_transactions = sales_df.select(\"Transaction_ID\").distinct().count()\n",
    "print(f\"Total Transactions: {total_transactions}\")\n",
    "\n",
    "# Avg Transaction Value by Store\n",
    "print(\"Average Transaction Value by Store\")\n",
    "sales_statistics = sales_df.groupBy(\"Store_ID\").agg(\n",
    "    count(\"Transaction_ID\").alias(\"Transaction_Count\"),\n",
    "    format_number(avg(\"Sales_Amount\"), 2).alias(\"Average_Sales_Amount\"),\n",
    "    format_number(sum(\"Sales_Amount\"), 2).alias(\"Total_Sales\")\n",
    ")\n",
    "sales_statistics.show()\n",
    "\n",
    "# Top 10 Products Sold by Sales\n",
    "print(\"Top 10 Products Sold by Sales Amount\")\n",
    "product_sales = sales_df.groupBy(\"Product_ID\").agg(\n",
    "    format_number(sum(col(\"Sales_Amount\")), 2).alias(\"Total_Sales\")\n",
    ")\n",
    "top_selling_products = product_sales.orderBy(col(\"Total_Sales\").desc()).limit(10)\n",
    "top_selling_products.show()\n",
    "\n",
    "### 5. Feedback Analysis\n",
    "print(\"=== Feedback Analysis ===\")\n",
    "feedback_df.printSchema()\n",
    "\n",
    "# Basic stats\n",
    "total_feedbacks = feedback_df.select(\"Feedback_ID\").distinct().count()\n",
    "print(f\"Total Feedbacks: {total_feedbacks}\")\n",
    "\n",
    "# Top 10 Products with Highest Ratings\n",
    "print(\"Top 10 Products with Highest Ratings\")\n",
    "product_feedback_stats = feedback_df.groupBy(\"Product_ID\").agg(\n",
    "    count(\"Feedback_ID\").alias(\"Feedback_Count\"),\n",
    "    format_number(avg(col(\"Feedback_Rating\")), 2).alias(\"Average_Rating\")\n",
    ")\n",
    "top_rated_products = product_feedback_stats.orderBy(col(\"Average_Rating\").desc()).limit(10)\n",
    "top_rated_products.show()\n",
    "\n",
    "# Feedback Rating Distribution\n",
    "print(\"Feedback Rating Distribution\")\n",
    "feedback_rating_distribution = feedback_df.groupBy(\"Feedback_Rating\").agg(\n",
    "    count(\"Feedback_ID\").alias(\"Feedback_Count\")\n",
    ")\n",
    "feedback_rating_distribution.orderBy(col(\"Feedback_Rating\").desc()).show()\n",
    "\n",
    "### 6. Loyalty Analysis\n",
    "print(\"=== Loyalty Analysis ===\")\n",
    "loyalty_df.printSchema()\n",
    "\n",
    "# Membership Tier Distribution\n",
    "print(\"Membership Tier Distribution\")\n",
    "membership_tier_distribution = loyalty_df.groupBy(\"Membership_Tier\").agg(\n",
    "    count(\"Customer_ID\").alias(\"Customer_Count\"),\n",
    "    format_number(avg(\"Points_Earned\"), 2).alias(\"Average_Points_Earned\"),\n",
    "    format_number(avg(\"Points_Redeemed\"), 2).alias(\"Average_Points_Redeemed\")\n",
    ")\n",
    "membership_tier_distribution.show()\n",
    "\n",
    "### 7. Relationship Analysis\n",
    "\n",
    "# Relationship 1: Sales and Products\n",
    "print(\"=== Relationship Analysis: Sales and Products ===\")\n",
    "sales_product_df = sales_df.join(product_df, \"Product_ID\").dropDuplicates([\"Product_ID\", \"Store_ID\"])\n",
    "\n",
    "sales_product_analysis = sales_product_df.groupBy(\"Category\", \"Brand_Name\").agg(\n",
    "    format_number(sum(\"Sales_Amount\"), 2).alias(\"Total_Sales\"),\n",
    "    format_number(avg(\"Sales_Amount\"), 2).alias(\"Average_Sales_Amount\")\n",
    ")\n",
    "sales_product_analysis.show()\n",
    "\n",
    "# Relationship 2: Sales and Stores\n",
    "print(\"=== Relationship Analysis: Sales and Stores ===\")\n",
    "sales_store_analysis = sales_df.groupBy(\"Store_ID\").agg(\n",
    "    format_number(sum(\"Sales_Amount\"), 2).alias(\"Total_Sales\"),\n",
    "    format_number(avg(\"Sales_Amount\"), 2).alias(\"Average_Sales_Amount\")\n",
    ")\n",
    "sales_store_analysis.show()\n",
    "\n",
    "# Relationship 3: Feedback and Products with Sales\n",
    "print(\"=== Relationship Analysis: Feedback and Products with Sales ===\")\n",
    "feedback_sales_product_df = feedback_df.join(sales_df, \"Product_ID\").join(product_df, \"Product_ID\").dropDuplicates([\"Product_ID\"])\n",
    "\n",
    "feedback_product_analysis = feedback_sales_product_df.groupBy(\"Product_ID\", \"Product_Name\").agg(\n",
    "    format_number(avg(\"Feedback_Rating\"), 2).alias(\"Average_Rating\"),\n",
    "    format_number(sum(\"Sales_Amount\"), 2).alias(\"Total_Sales\")\n",
    ")\n",
    "feedback_product_analysis.orderBy(col(\"Average_Rating\").desc()).show(10)\n",
    "\n",
    "# Relationship 4: Loyalty and Sales\n",
    "print(\"=== Relationship Analysis: Loyalty and Sales ===\")\n",
    "loyalty_sales_df = loyalty_df.join(sales_df, \"Customer_ID\").dropDuplicates([\"Customer_ID\"])\n",
    "\n",
    "loyalty_sales_analysis = loyalty_sales_df.groupBy(\"Membership_Tier\").agg(\n",
    "    format_number(sum(\"Sales_Amount\"), 2).alias(\"Total_Sales\"),\n",
    "    format_number(avg(\"Sales_Amount\"), 2).alias(\"Average_Sales_Amount\")\n",
    ")\n",
    "loyalty_sales_analysis.show()\n",
    "\n",
    "# Relationship 5: Age and Sales\n",
    "print(\"=== Relationship Analysis: Age and Sales ===\")\n",
    "age_sales_df = customer_age_df.join(sales_df, \"Customer_ID\").dropDuplicates([\"Customer_ID\"])\n",
    "\n",
    "age_sales_analysis = age_sales_df.groupBy(\"Age\").agg(\n",
    "    format_number(sum(\"Sales_Amount\"), 2).alias(\"Total_Sales\"),\n",
    "    format_number(avg(\"Sales_Amount\"), 2).alias(\"Average_Sales_Amount\")\n",
    ")\n",
    "age_sales_analysis.orderBy(col(\"Age\").desc()).show()\n",
    "\n",
    "### 8. Summary Statistics for Customer Data\n",
    "print(\"=== Basic Statistics for Customer Data ===\")\n",
    "customer_df.describe().show()\n",
    "\n",
    "print(\"=== Top 10 Cities by Customer Count ===\")\n",
    "customer_df.groupBy(\"City\").count().orderBy(col(\"count\").desc()).show(10)\n",
    "\n",
    "print(\"=== Top 10 States by Customer Count ===\")\n",
    "customer_df.groupBy(\"State\").count().orderBy(col(\"count\").desc()).show(10)\n",
    "\n",
    "### 9. Aggregated Sales Data for Customers\n",
    "print(\"=== Aggregated Sales Data for Customers ===\")\n",
    "sales_agg_df = sales_df.groupBy(\"Customer_ID\").agg(\n",
    "    sum(\"Sales_Amount\").alias(\"Total_Spend\"),\n",
    "    countDistinct(\"Transaction_ID\").alias(\"Total_Transactions\"),\n",
    "    max(\"Date\").alias(\"Last_Purchase_Date\"),\n",
    "    min(\"Date\").alias(\"First_Purchase_Date\")\n",
    ")\n",
    "\n",
    "# Calculate Recency in days\n",
    "sales_agg_df = sales_agg_df.withColumn(\"Recency\", datediff(current_date(), col(\"Last_Purchase_Date\")))\n",
    "\n",
    "# Join with customer data\n",
    "customer_sales_df = customer_df.join(sales_agg_df, \"Customer_ID\", \"left\")\n",
    "\n",
    "# Show summary statistics of the new DataFrame\n",
    "print(\"=== Summary Statistics of Customer Spend, Transactions, and Recency ===\")\n",
    "customer_sales_df.describe([\"Total_Spend\", \"Total_Transactions\", \"Recency\"]).show()\n",
    "\n",
    "# Display the first few rows of the final DataFrame for review\n",
    "print(\"=== Sample Data from Customer Sales DataFrame ===\")\n",
    "customer_sales_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028ff4d9-0325-4fc9-b681-6b9c7d273d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import sum, countDistinct, max, datediff, current_date, mean, col, avg, year\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, StringIndexer\n",
    "from pyspark.ml.clustering import KMeans, BisectingKMeans, GaussianMixture\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"Customer Segmentation\").getOrCreate()\n",
    "\n",
    "# Function to load data\n",
    "def read_dataframe_from_sqlite(spark, table_name, db_path, driver):\n",
    "    return spark.read.format(\"jdbc\").option(\"url\", f\"jdbc:sqlite:{db_path}\")\\\n",
    "            .option(\"dbtable\", table_name).option(\"driver\", driver).load()\n",
    "\n",
    "# Load and preprocess data\n",
    "def load_data(spark):\n",
    "    customer_df = read_dataframe_from_sqlite(spark, \"customer_sdf\", \"../data/raw/retail_data.db\",\"org.sqlite.JDBC\")\n",
    "    sales_df = read_dataframe_from_sqlite(spark, \"sales_sdf\", \"../data/raw/retail_data.db\",\"org.sqlite.JDBC\")\n",
    "    product_df = read_dataframe_from_sqlite(spark, \"product_sdf\", \"../data/raw/retail_data.db\",\"org.sqlite.JDBC\")\n",
    "    store_df = read_dataframe_from_sqlite(spark, \"store_sdf\", \"../data/raw/retail_data.db\",\"org.sqlite.JDBC\")\n",
    "    loyalty_df = read_dataframe_from_sqlite(spark, \"loyalty_sdf\", \"../data/raw/retail_data.db\",\"org.sqlite.JDBC\")\n",
    "    return customer_df, sales_df, product_df, store_df, loyalty_df\n",
    "\n",
    "# Calculate RFM metrics\n",
    "def calculate_rfm(sales_df):\n",
    "    rfm_metrics = sales_df.groupBy(\"Customer_ID\").agg(\n",
    "        datediff(current_date(), max(\"Date\")).alias(\"Recency\"),\n",
    "        countDistinct(\"Transaction_ID\").alias(\"Frequency\"),\n",
    "        sum(\"Sales_Amount\").alias(\"Monetary\")\n",
    "    )\n",
    "    return rfm_metrics\n",
    "\n",
    "# Preprocess data (assemble and scale features)\n",
    "def preprocess_data(customer_rfm_df, feature_columns):\n",
    "    # Cast BigDecimal columns to DoubleType\n",
    "    for feature in feature_columns:\n",
    "        customer_rfm_df = customer_rfm_df.withColumn(feature, col(feature).cast(DoubleType()))\n",
    "    \n",
    "    # Check if DataFrame is empty\n",
    "    if customer_rfm_df.count() == 0:\n",
    "        raise ValueError(\"The DataFrame is empty. Cannot proceed with preprocessing.\")\n",
    "    \n",
    "    # Calculate mean values with error handling for None\n",
    "    mean_values = {}\n",
    "    for feature in feature_columns:\n",
    "        mean_value = customer_rfm_df.select(mean(feature)).first()[0]\n",
    "        if mean_value is None:\n",
    "            mean_value = 0.0  # Default to 0.0 if the mean is None\n",
    "        mean_values[feature] = mean_value\n",
    "\n",
    "    # Fill missing values with calculated mean values\n",
    "    customer_rfm_df = customer_rfm_df.fillna(mean_values)\n",
    "\n",
    "    # Assemble features into a vector\n",
    "    assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\", handleInvalid=\"skip\")\n",
    "    feature_vector = assembler.transform(customer_rfm_df)\n",
    "    \n",
    "    # Check if features contain any data\n",
    "    if feature_vector.select(\"features\").head()[0].size == 0:\n",
    "        raise ValueError(\"Feature vector is empty. Cannot proceed with scaling.\")\n",
    "    \n",
    "    # Apply Standard Scaler\n",
    "    scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withMean=True, withStd=True)\n",
    "    scaled_data = scaler.fit(feature_vector).transform(feature_vector)\n",
    "    \n",
    "    return scaled_data\n",
    "\n",
    "# Train and evaluate clustering model\n",
    "def train_and_evaluate_model(model, scaled_data, model_name, feature_columns):\n",
    "    # Check if DataFrame is empty\n",
    "    if scaled_data.count() == 0:\n",
    "        raise ValueError(f\"The DataFrame for {model_name} is empty. Cannot proceed with model training.\")\n",
    "    \n",
    "    trained_model = model.fit(scaled_data)\n",
    "    clusters = trained_model.transform(scaled_data)\n",
    "    \n",
    "    # Check if the number of clusters is greater than one\n",
    "    num_clusters = clusters.select(\"prediction\").distinct().count()\n",
    "    if num_clusters <= 1:\n",
    "        print(f\"{model_name}: Number of clusters is {num_clusters}. Cannot compute silhouette score.\")\n",
    "        return None, clusters\n",
    "    \n",
    "    evaluator = ClusteringEvaluator(featuresCol=\"scaled_features\", metricName=\"silhouette\", distanceMeasure=\"squaredEuclidean\")\n",
    "    silhouette = evaluator.evaluate(clusters)\n",
    "    \n",
    "    print(f\"{model_name} Silhouette Score: {silhouette}\")\n",
    "    \n",
    "    # Dynamically aggregate based on available features\n",
    "    agg_exprs = [avg(col(feature)).alias(f\"Avg_{feature}\") for feature in feature_columns]\n",
    "    \n",
    "    cluster_summary = clusters.groupBy(\"prediction\").agg(*agg_exprs)\n",
    "    cluster_summary.show()\n",
    "    \n",
    "    return silhouette, clusters\n",
    "\n",
    "# Product Purchase Segmentation\n",
    "def product_purchase_segmentation(sales_df, product_df):\n",
    "    sales_product_df = sales_df.join(product_df, \"Product_ID\")\n",
    "    customer_product_metrics = sales_product_df.groupBy(\"Customer_ID\", \"Category\").agg(\n",
    "        countDistinct(\"Transaction_ID\").alias(\"Purchase_Count\"),\n",
    "        sum(\"Sales_Amount\").alias(\"Total_Spending\")\n",
    "    )\n",
    "    return customer_product_metrics\n",
    "\n",
    "# Store Visit Segmentation\n",
    "def store_visit_segmentation(sales_df, store_df):\n",
    "    # Perform the join with the condition specified\n",
    "    sales_store_df = sales_df.join(store_df, sales_df[\"Store_ID\"] == store_df[\"Store_Location\"], \"left\")\n",
    "    \n",
    "    # Group by Customer_ID and Store_Type to calculate visit frequency and total spending\n",
    "    customer_store_metrics = sales_store_df.groupBy(\"Customer_ID\", \"Store_Type\").agg(\n",
    "        countDistinct(\"Transaction_ID\").alias(\"Visit_Frequency\"),\n",
    "        sum(\"Sales_Amount\").alias(\"Total_Spending\")\n",
    "    )\n",
    "    return customer_store_metrics\n",
    "\n",
    "# Demographic Segmentation with String Indexing\n",
    "def demographic_segmentation(customer_df):\n",
    "    customer_df = customer_df.withColumn(\"Age\", year(current_date()) - year(customer_df[\"DOB\"]))\n",
    "\n",
    "    # Convert categorical string columns to numerical indices\n",
    "    indexers = [StringIndexer(inputCol=col, outputCol=col+\"_index\").fit(customer_df) for col in [\"Gender\", \"City\", \"State\"]]\n",
    "    for indexer in indexers:\n",
    "        customer_df = indexer.transform(customer_df)\n",
    "    \n",
    "    return customer_df\n",
    "\n",
    "# Loyalty Program Segmentation\n",
    "def loyalty_program_segmentation(loyalty_df):\n",
    "    return loyalty_df\n",
    "\n",
    "# Load data\n",
    "customer_df, sales_df, product_df, store_df, loyalty_df = load_data(spark)\n",
    "\n",
    "# RFM Segmentation\n",
    "rfm_df = calculate_rfm(sales_df)\n",
    "customer_rfm_df = customer_df.join(rfm_df, \"Customer_ID\", \"left\")\n",
    "scaled_data_rfm = preprocess_data(customer_rfm_df, [\"Recency\", \"Frequency\", \"Monetary\"])\n",
    "\n",
    "# Product Purchase Segmentation\n",
    "product_metrics_df = product_purchase_segmentation(sales_df, product_df)\n",
    "scaled_data_product = preprocess_data(product_metrics_df, [\"Purchase_Count\", \"Total_Spending\"])\n",
    "\n",
    "# Store Visit Segmentation\n",
    "store_metrics_df = store_visit_segmentation(sales_df, store_df)\n",
    "print(\"Store Metrics DataFrame:\")\n",
    "store_metrics_df.show()  # Inspect the DataFrame\n",
    "\n",
    "if store_metrics_df.count() == 0:\n",
    "    print(\"No data available for store visit segmentation.\")\n",
    "else:\n",
    "    scaled_data_store = preprocess_data(store_metrics_df, [\"Visit_Frequency\", \"Total_Spending\"])\n",
    "\n",
    "# Demographic Segmentation\n",
    "demographic_df = demographic_segmentation(customer_df)\n",
    "scaled_data_demographic = preprocess_data(demographic_df, [\"Age\", \"Gender_index\", \"City_index\", \"State_index\"])\n",
    "\n",
    "# Loyalty Program Segmentation\n",
    "loyalty_df = loyalty_program_segmentation(loyalty_df)\n",
    "scaled_data_loyalty = preprocess_data(loyalty_df, [\"Points_Earned\", \"Points_Redeemed\", \"Membership_Tier\"])\n",
    "\n",
    "# Dictionary to store models and their names\n",
    "models = {\n",
    "    \"KMeans\": KMeans(k=4, seed=1, featuresCol=\"scaled_features\"),\n",
    "    \"BisectingKMeans\": BisectingKMeans(k=4, seed=1, featuresCol=\"scaled_features\"),\n",
    "    \"GaussianMixture\": GaussianMixture(k=4, seed=1, featuresCol=\"scaled_features\")\n",
    "}\n",
    "\n",
    "# Train and evaluate each model for each segmentation\n",
    "segmentation_data = {\n",
    "    \"RFM\": (scaled_data_rfm, [\"Recency\", \"Frequency\", \"Monetary\"]),\n",
    "    \"Product Purchase\": (scaled_data_product, [\"Purchase_Count\", \"Total_Spending\"]),\n",
    "    \"Store Visit\": (scaled_data_store if 'scaled_data_store' in locals() else None, [\"Visit_Frequency\", \"Total_Spending\"]),\n",
    "    \"Demographics\": (scaled_data_demographic, [\"Age\", \"Gender_index\", \"City_index\", \"State_index\"]),\n",
    "    \"Loyalty Program\": (scaled_data_loyalty, [\"Points_Earned\", \"Points_Redeemed\", \"Membership_Tier\"])\n",
    "}\n",
    "\n",
    "best_models = {}\n",
    "\n",
    "for seg_name, (data, feature_columns) in segmentation_data.items():\n",
    "    if data is not None and data.count() > 0:\n",
    "        silhouette_scores = {}\n",
    "        cluster_results = {}\n",
    "        best_model_name = None\n",
    "        best_score = float('-inf')\n",
    "        \n",
    "        for model_name, model in models.items():\n",
    "            silhouette_score, clusters = train_and_evaluate_model(model, data, f\"{seg_name} - {model_name}\", feature_columns)\n",
    "            if silhouette_score is not None:  # Only consider valid silhouette scores\n",
    "                silhouette_scores[model_name] = silhouette_score\n",
    "                cluster_results[model_name] = clusters\n",
    "                \n",
    "                if silhouette_score > best_score:\n",
    "                    best_score = silhouette_score\n",
    "                    best_model_name = model_name\n",
    "        \n",
    "        if best_model_name is not None:\n",
    "            best_models[seg_name] = {\n",
    "                \"model_name\": best_model_name,\n",
    "                \"silhouette_score\": best_score,\n",
    "                \"clusters\": cluster_results[best_model_name]\n",
    "            }\n",
    "            print(f\"Best Model for {seg_name}: {best_model_name} with Silhouette Score = {best_score}\")\n",
    "        else:\n",
    "            print(f\"No valid clustering model found for {seg_name}.\")\n",
    "    else:\n",
    "        print(f\"No data available for segmentation: {seg_name}\")\n",
    "\n",
    "# Visualize clusters for the best RFM model\n",
    "if \"RFM\" in best_models:\n",
    "    rfm_clusters = best_models[\"RFM\"][\"clusters\"]\n",
    "    pandas_df = rfm_clusters.select(\"Recency\", \"Frequency\", \"Monetary\", \"prediction\").toPandas()\n",
    "    \n",
    "    sns.pairplot(pandas_df, hue=\"prediction\", palette=\"viridis\")\n",
    "    plt.show()\n",
    "\n",
    "# End Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5423dd-583c-46b6-9983-9b3d77cb221a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import sqlite3\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window  # Importing Window\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, StringIndexer\n",
    "from pyspark.ml.clustering import KMeans, BisectingKMeans, GaussianMixture\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Initialize Spark session\n",
    "import sys\n",
    "import os\n",
    "notebook_dir = os.getcwd()\n",
    "src_path = os.path.abspath(os.path.join(notebook_dir, '..', 'src'))\n",
    "sys.path.append(src_path)\n",
    "\n",
    "from utils.spark_utils import load_config, initialize_spark, insert_dataframe_to_sqlite, read_dataframe_from_sqlite\n",
    "from pyspark.sql.functions import col, count, avg, sum, format_number,current_date\n",
    "\n",
    "# Load configuration\n",
    "config = load_config(\"../configs/data_config.json\")\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = initialize_spark(config[\"spark\"][\"app_name\"], \"../jars/sqlite-jdbc-3.46.0.1.jar\")\n",
    "\n",
    "def load_data(db_path):\n",
    "    return (\n",
    "        read_dataframe_from_sqlite(\"customer_sdf\", db_path),\n",
    "        read_dataframe_from_sqlite(\"sales_sdf\", db_path),\n",
    "        read_dataframe_from_sqlite(\"product_sdf\", db_path),\n",
    "        read_dataframe_from_sqlite(\"store_sdf\", db_path),\n",
    "        read_dataframe_from_sqlite(\"loyalty_sdf\", db_path),\n",
    "        read_dataframe_from_sqlite(\"feedback_sdf\", db_path)\n",
    "    )\n",
    "\n",
    "def calculate_rfm_with_time(sales_df):\n",
    "    # Ensure Date is properly formatted and retained before aggregation\n",
    "    sales_df = sales_df.withColumn(\"Date\", to_date(col(\"Date\"), \"yyyy-MM-dd\"))\n",
    "    \n",
    "    # Window specification to calculate the earliest date for each customer\n",
    "    windowSpec = Window.partitionBy(\"Customer_ID\")\n",
    "    \n",
    "    # Calculate maximum purchase date, frequency, and monetary while retaining Date for further operations\n",
    "    rfm_df = sales_df.groupBy(\"Customer_ID\").agg(\n",
    "        max(\"Date\").alias(\"Last_Purchase_Date\"),\n",
    "        min(\"Date\").alias(\"First_Purchase_Date\"),\n",
    "        countDistinct(\"Transaction_ID\").alias(\"Frequency\"),\n",
    "        sum(\"Sales_Amount\").alias(\"Monetary\")\n",
    "    )\n",
    "\n",
    "    # Calculate Recency and Customer Age\n",
    "    rfm_df = rfm_df.withColumn(\"Recency\", datediff(current_date(), col(\"Last_Purchase_Date\")))\n",
    "    rfm_df = rfm_df.withColumn(\"Customer_Age\", datediff(current_date(), col(\"First_Purchase_Date\")))\n",
    "    \n",
    "    return rfm_df\n",
    "\n",
    "# Add other segmentation functions here\n",
    "def loyalty_program_engagement_segmentation(loyalty_df):\n",
    "    # Assuming Loyalty dataframe has necessary fields\n",
    "    indexer = StringIndexer(inputCol=\"Membership_Tier\", outputCol=\"Membership_Tier_Index\")\n",
    "    loyalty_df = loyalty_df.withColumn(\"Points_Redeemed\", col(\"Points_Redeemed\").cast(DoubleType()))\n",
    "    loyalty_df = loyalty_df.groupBy(\"Customer_ID\").agg(\n",
    "        sum(\"Points_Earned\").alias(\"Total_Points_Earned\"),\n",
    "        sum(\"Points_Redeemed\").alias(\"Total_Points_Redeemed\"),\n",
    "        first(\"Membership_Tier\").alias(\"Membership_Tier\")\n",
    "    )\n",
    "    return indexer.fit(loyalty_df).transform(loyalty_df)\n",
    "\n",
    "def customer_lifecycle_segmentation(rfm_df):\n",
    "    # Adding a lifecycle segment based on recency\n",
    "    lifecycle_df = rfm_df.withColumn(\n",
    "        \"Lifecycle_Segment\",\n",
    "        when(col(\"Recency\") <= 30, \"Active\")\n",
    "        .when((col(\"Recency\") > 30) & (col(\"Recency\") <= 90), \"Warm\")\n",
    "        .otherwise(\"Inactive\")\n",
    "    )\n",
    "    # Convert categorical column to numeric using StringIndexer\n",
    "    indexer = StringIndexer(inputCol=\"Lifecycle_Segment\", outputCol=\"Lifecycle_Segment_Index\")\n",
    "    lifecycle_df = indexer.fit(lifecycle_df).transform(lifecycle_df)\n",
    "    return lifecycle_df\n",
    "\n",
    "\n",
    "def product_affinity_segmentation(sales_df, product_df):\n",
    "    # Assuming sales_df and product_df have been joined and necessary aggregations have been made\n",
    "    return sales_df.join(product_df, \"Product_ID\").groupBy(\"Customer_ID\", \"Category\").agg(\n",
    "        countDistinct(\"Transaction_ID\").alias(\"Category_Purchase_Count\"),\n",
    "        sum(\"Sales_Amount\").alias(\"Category_Spending\")\n",
    "    )\n",
    "\n",
    "def store_loyalty_segmentation(sales_df, store_df):\n",
    "    # Assuming necessary data is available in store_df and has been appropriately joined\n",
    "    return sales_df.join(store_df, \"Store_ID\").groupBy(\n",
    "        \"Customer_ID\", \"Store_Type\").agg(\n",
    "        countDistinct(\"Transaction_ID\").alias(\"Store_Visit_Frequency\"),\n",
    "        sum(\"Sales_Amount\").alias(\"Store_Total_Spending\")\n",
    "    )\n",
    "\n",
    "def feedback_sentiment_segmentation(feedback_df):\n",
    "    # Assume feedback_df contains 'Customer_ID' and 'Feedback_Rating'\n",
    "    return feedback_df.groupBy(\"Customer_ID\").agg(\n",
    "        avg(\"Feedback_Rating\").alias(\"Avg_Sentiment_Score\")\n",
    "    )\n",
    "    \n",
    "def read_dataframe_from_sqlite(table_name, db_path):\n",
    "    logging.info(f\"Reading DataFrame from SQLite: Table={table_name}, DB={db_path}\")\n",
    "    return spark.read.format(\"jdbc\").option(\"url\", f\"jdbc:sqlite:{db_path}\").option(\"dbtable\", table_name).option(\"driver\", \"org.sqlite.JDBC\").load()\n",
    "\n",
    "def write_df_to_sqlite(df, table_name, db_path):\n",
    "    logging.info(f\"Writing DataFrame to SQLite: Table={table_name}, DB={db_path}\")\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    df.to_sql(table_name, conn, if_exists='append', index=False)\n",
    "    conn.close()\n",
    "\n",
    "def get_next_version_number(model_name, db_path):\n",
    "    logging.info(f\"Fetching next version number for model: {model_name}\")\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT MAX(Version) FROM model_info WHERE Model_Name = ?\", (model_name,))\n",
    "    result = cursor.fetchone()\n",
    "    next_version = (result[0] if result[0] is not None else 0) + 1\n",
    "    conn.close()\n",
    "    return next_version\n",
    "\n",
    "def save_spark_model(model, model_name, db_path):\n",
    "    logging.info(f\"Saving Spark model: {model_name}\")\n",
    "    version = get_next_version_number(model_name, db_path)\n",
    "    date_path = datetime.now().strftime(\"%Y%m%d\")\n",
    "    base_path = f\"../data/results/models/{date_path}\"\n",
    "    os.makedirs(base_path, exist_ok=True)\n",
    "    file_path = f\"{base_path}/{version}/{model_name}_v{version}\"\n",
    "    model.save(file_path)\n",
    "    logging.info(f\"Model saved at {file_path} with version {version}\")\n",
    "    return file_path, version\n",
    "    \n",
    "def save_model_info_to_db(model_name, file_path, db_path, silhouette_score, version, training_time, k_value):\n",
    "    logging.info(f\"Saving model info to database for {model_name}, version {version}\")\n",
    "    timestamp = datetime.now()\n",
    "    model_info_df = pd.DataFrame([(model_name, file_path, timestamp, silhouette_score, version, training_time, k_value)],\n",
    "                                 columns=[\"Model_Name\", \"File_Path\", \"Saved_At\", \"Silhouette_Score\", \"Version\", \"Training_Time\", \"K_Value\"])\n",
    "    write_df_to_sqlite(model_info_df, \"model_info\", db_path)\n",
    "\n",
    "def preprocess_and_cluster(data, feature_columns, k_values, model_types, segmentation_name, db_path):\n",
    "    logging.info(f\"Starting preprocessing and clustering for {segmentation_name}\")\n",
    "    if data.rdd.isEmpty():\n",
    "        logging.warning(f\"No data available for segmentation: {segmentation_name}\")\n",
    "        return None\n",
    "\n",
    "    assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "    data = assembler.transform(data.na.fill(0))\n",
    "\n",
    "    scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
    "    data = scaler.fit(data).transform(data)\n",
    "\n",
    "    best_score = -1\n",
    "    best_model_details = None\n",
    "\n",
    "    for k in k_values:\n",
    "        for model_type in model_types:\n",
    "            start_time = time.time()\n",
    "            model = model_types[model_type](k=k, featuresCol=\"scaled_features\")\n",
    "            logging.info(f\"Training {model_type} with k={k} for {segmentation_name}\")\n",
    "            try:\n",
    "                trained_model = model.fit(data)\n",
    "                predictions = trained_model.transform(data)\n",
    "                training_time = time.time() - start_time\n",
    "                evaluator = ClusteringEvaluator(featuresCol=\"scaled_features\", metricName=\"silhouette\")\n",
    "                silhouette = evaluator.evaluate(predictions)\n",
    "                if silhouette > best_score:\n",
    "                    best_score = silhouette\n",
    "                    file_path, version = save_spark_model(trained_model, f\"{segmentation_name}-{model_type}-k{k}\", db_path)\n",
    "                    best_model_details = (model_type, k, file_path, silhouette, predictions, training_time, version)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Failed to fit or transform model {model_type} for {segmentation_name} with k={k}: {str(e)}\")\n",
    "\n",
    "    if best_model_details:\n",
    "        model_type, k, file_path, silhouette, best_predictions, training_time, version = best_model_details\n",
    "        save_model_info_to_db(f\"{segmentation_name}-{model_type}-k{k}\", file_path, db_path, silhouette, version, training_time, k)\n",
    "        best_predictions = best_predictions.withColumnRenamed(\"prediction\", \"cluster\")\n",
    "        return best_predictions.withColumn(\"Timestamp\", current_timestamp())\n",
    "\n",
    "    return None\n",
    "\n",
    "def main():\n",
    "    db_path = \"../data/raw/retail_data.db\"\n",
    "    # Load data\n",
    "    customer_df, sales_df, product_df, store_df, loyalty_df, feedback_df = load_data(db_path)\n",
    "\n",
    "    # Define the segmentations\n",
    "    segmentations = {\n",
    "        \"RFM_with_Time\": {\n",
    "            \"data_func\": lambda: calculate_rfm_with_time(sales_df),\n",
    "            \"features\": [\"Recency\", \"Frequency\", \"Monetary\", \"Customer_Age\"]\n",
    "        },\n",
    "        \"Loyalty_Program_Engagement\": {\n",
    "            \"data_func\": lambda: loyalty_program_engagement_segmentation(loyalty_df),\n",
    "            \"features\": [\"Total_Points_Earned\", \"Total_Points_Redeemed\", \"Membership_Tier_Index\"]\n",
    "        },\n",
    "        \"Customer_Lifecycle\": {\n",
    "            \"data_func\": lambda: customer_lifecycle_segmentation(calculate_rfm_with_time(sales_df)),\n",
    "            \"features\": [\"Recency\", \"Lifecycle_Segment_Index\"]\n",
    "        },\n",
    "        \"Product_Affinity\": {\n",
    "            \"data_func\": lambda: product_affinity_segmentation(sales_df, product_df),\n",
    "            \"features\": [\"Category_Purchase_Count\", \"Category_Spending\"]\n",
    "        },\n",
    "        \"Store_Loyalty\": {\n",
    "            \"data_func\": lambda: store_loyalty_segmentation(sales_df, store_df),\n",
    "            \"features\": [\"Store_Visit_Frequency\", \"Store_Total_Spending\"]\n",
    "        },\n",
    "        \"Feedback_Sentiment\": {\n",
    "            \"data_func\": lambda: feedback_sentiment_segmentation(feedback_df),\n",
    "            \"features\": [\"Avg_Sentiment_Score\"]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Define model types and k values for clustering\n",
    "    model_types = {\n",
    "        \"KMeans\": KMeans,\n",
    "        \"BisectingKMeans\": BisectingKMeans,\n",
    "        \"GaussianMixture\": GaussianMixture\n",
    "    }\n",
    "    k_values = [2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "    # Process each segmentation\n",
    "    for seg_name, seg_info in segmentations.items():\n",
    "        logging.info(f\"Processing segmentation: {seg_name}\")\n",
    "        seg_data = seg_info[\"data_func\"]()\n",
    "        predictions = preprocess_and_cluster(seg_data, seg_info[\"features\"], k_values, model_types, seg_name, db_path)\n",
    "        if predictions:\n",
    "            predictions_pandas = predictions.select(\"Customer_ID\", \"cluster\", \"Timestamp\").toPandas()\n",
    "            write_df_to_sqlite(predictions_pandas, f\"{seg_name}_results\", db_path)\n",
    "    spark.stop()\n",
    "\n",
    "\n",
    "def create_or_update_table(db_path):\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS model_info (\n",
    "            Model_Name TEXT,\n",
    "            File_Path TEXT,\n",
    "            Saved_At TIMESTAMP,\n",
    "            Silhouette_Score FLOAT,\n",
    "            Version INTEGER,\n",
    "            Training_Time FLOAT,\n",
    "            K_Value INTEGER\n",
    "        );\n",
    "    ''')\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    logging.info(\"Database table 'model_info' checked or updated.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    create_or_update_table(\"../data/raw/retail_data.db\")  # Ensure DB schema is correct\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
